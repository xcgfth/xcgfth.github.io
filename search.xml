<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[自然语言处理面试常见问题]]></title>
    <url>%2F2019%2F05%2F08%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1 RNN是怎么运行的？2 什么是char-RNN，RNN结构图。3 Seq2Seq的结构。3 Attention的结构4 机器学习常用的分类算法：Logistic Regression, SVM, 决策树，随机森林等相关分类算法的原理，公式推导，模型评价，模型调参，使用场景。5 机器学习常用的聚类算法：K-Means, BDSCAN, SOM, LDA等算法的原理，模型参数的确定以及确定的方法，模型的评价，模型的使用场景（例如LDA应该确定几个主题，K-Means的k如何确定，DBSCAN密度可达与密度直达）。6 特征工程： 特征选择，特征提取，PCA降维方法中的参数主成分的确定方法，如何进行特征选择。7 Boosting和Bagging的区别。8 数据如何去除噪声，如何找到离群点，异常值，现有机器学习算法哪些可以去除噪声。9 HMM与N-gram模型之间的区别。10 梯度消失与梯度爆炸。11 奥卡姆剃须刀原理。12 TCP三次握手的原理，为什么是三次而不是其他次。13 进行数据处理时，如何过滤无用的信息，数据乱码的处理。14 交叉熵与信息熵，信息增益与信息增益率，gini系数，具体如何计算。15 BIC准则（贝叶斯信息准则）与AIC（赤池信息准则）。16 前向传播与反向传播。17 常见的损失函数。18 请列出几种文本特征提取算法。 参考答案： 文档频率、信息增益、互信息、$\chi^2$统计、TF-IDF19 简述几种自然语言处理开源工具包。 参考答案：LingPipe、FudanNLP、OpenNLP、CRF++、Standord CoreNLP、IKAnalyzer20 简述无监督和有监督算法的区别。 参考答案：（1）有监督学习：对具有概念标记（分类）的训练样本进行学习，以尽可能对训练样本集外的数据进行标记（分类）预测。这里，所有的标记（分类）是已知的。因此，训练样本的岐义性低。无监督学习：对没有概念标记（分类）的训练样本进行学习，以发现训练样本集中的结构性知识。这里，所有的标记（分类）是未知的。因此，训练样本的岐义性高。聚类就是典型的无监督学习（2）有监督学习的样本全部带标记，无监督学习的样本全部不带标记。PS:部分带标记的是半监督学习（3）训练集有输入有输出是有监督，包括所有的回归算法分类算法，比如线性回归、决策树、神经网络、KNN、SVM等；训练集只有输入没有输出是无监督，包括所有的聚类算法，比如k-means 、PCA、 GMM等21 深度学习如何提取query特征，如何利用深度学习计算语义相似度。22 算法题： 写二叉树的前序遍历，中序遍历，统计二叉树所有路径和。22 RNN为什么会梯度消失，LSTM怎么能解决梯度消失问题。23 介绍一下常见优化算法及其特点。24 Dropout的原理。25 交叉熵损失函数是什么。26 介绍一下Word2vec，CBOW和Skip-gram的区别是什么。27 GBDT和Xgboost介绍一下，并说一下区别。28 算法题：现在有词向量词典，计算句子相似度（Consine Similarity）。29 介绍一下随机森林和Xgboost，有什么区别（从bagging和boosting角度）。30 什么是SGD，什么是batch size。31 深度学习优化算法有哪些，随便介绍一个。32 现有一个神经网络和64个样本，Batch gradient descent和SGD的时间复杂度和效果比较；采用批梯度下降时，神经网络参数更新了几次。33 font color=red&gt;算法题： Two Sum问题。34 font color=red&gt;算法题：如何找到10万以内的所有质数。35 Logistic回归的损失函数怎么来的，如何进行梯度更新。36 Xgboost原理，xgboost有哪些参数，怎么调整xgboost的参数。37 现在有三枚硬币，一个是一正一反，一个是两面都是正，一个是两面都是反，现在随机抛出一枚硬币是正面，那么这枚硬币的反面也是正面的概率。 参考答案：$\frac{2}{3}$38 现在有一个比较小的数据表（包括id, score），另外有一个十分大的（上千万级别）的数据表（包括id, name），现在需要以id为索引将两张表合并，如何在O(n)时间复杂度完成。39 GRU、LSTM以及RNN的区别在什么地方。40 GBDT的损失函数是什么。41 红黑树。42 64匹马，8个跑道，选出速度最快的4匹马需要多少次。 参考答案：1143 介绍一下LSTM（介绍LSTM时候提到RNN，打断询问RNN为什么有梯度消失问题，给出具体公式）。44 LSTM用什么框架实现的，能不能介绍一下Word2ver如何使用在其中，使用Word2vec和不使用word2vec的效果如何。45 正则化方法有哪些，介绍一下（说到L1和L2时，重点问了一下为什么梯度稀疏和梯度选择，用公式推导讲了一下）。46 机器学习、数据挖掘和深度学习的区别。47 算法题：二叉搜索树的插入和搜索。48 有序循环链表中（后简化为元素从小到大有序循环链表），如何在O(1)时间内完成最大值插入。49 算法题： 写代码实现列表 [0,0,6,2,8,0,0] —-&gt; [6,2,8,0,0,0]，要求O(n)时间复杂度和O(1)空间复杂度。50 LSTM用来解决RNN的什么问题？如何解决的？既然说到forget gate，那么说一下forget gate的取值范围？（sigmoid 取值（0,1））forget gate是具体的值还是向量？（向量），如何理解这个向量？51 深度学习用的什么框架，Tensorflow？（Keras），那介绍一下深度学习中的过拟合如何解决？（从数据、单模型、模型集成三个角度回答）。52 深度学习优化算法用过哪些？讲讲Sgd和gd的区别？53 对SVM（考虑线性可分情况）、LR和DT熟悉么？ 从损失函数说一下区别，SVM的损失函数是什么？（合页损失函数，写一下讲一下）。LR呢？（利用最大似然估计得出）。又问一下SVM线性可分情况下决策边界不同位置的损失值。DT如何进行特征选择？（ID3信息增益）。介绍一下信息熵？（随机变量不确定性，度量系统稳定性） [1/3,1/3,1/3]和[1/2,1/4,1/4]哪个的信息熵大？回归任务中如何进行特征选择？（平方损失准则）。54 海量数据处理。现在有1千万行词，需要统计各个词出现的次数，目前有一台机器内存1G，磁盘100G？（海量数据处理blog的第一题，先利用Hash对原始文本进行分割（hash(word)%2000，分为2000个文件），再使用hashmap（python中的字典）在各个文件中分别统计）。55 LTR（learning to rank）。介绍一下ltr的三种方式？其中pairwise在训练时怎么做？（转化为二分类）在测试的时候怎么做？56 文本分类的项目中用到CNN没有？介绍一下CNN？那CNN在文本分类任务中卷积核和一般的图像任务中的卷积核有什么区别？57 熟悉Attention么，介绍一下。58 在你的项目中如何判断word2vec的效果好坏，如何评判对模型和结果的影响？项目中使用的xgboost是哪个版本的？谁写的？59 如何从概率角度理解AUC？二分类问题中，一个正负类比是1:1000，一个是1:100，它们的AUC和ROC有什么区别？60 了解交叉熵损失函数么？在哪个场景使用过？它和最大似然估计是什么关系？61 算法题： 覆盖字符串所有字符的最小字串。62 算法题：反转链表的前k个。63 算法题：求二叉树最大深度。64 序列模型中markov和rnn的区别。 参考答案：rnn和hmm最本质的区别在于rnn没有马尔科夫假设，因此从理论上可以考虑很久的信息；同时hmm本质上是一个概率模型，而rnn不是；此外rnn具备神经网络的拟合非线性的能力。65 算法题：字符串出现第k多的字符。66 一个过拟合模型和大量数据，如何判断这些数据有没有用？67 lr和svm的区别。 参考答案：从lr的由来讲损失函数，对于svm讲最大间隔。区别在于损失函数不同；svm只需要考虑支持向量，而lr需要考虑所有的点；svm本质上是基于距离的，因此其输出无法直接产生概率，lr输出的是其属于分类的概率；在非线性的情况下，svm使用核函数解决，而lr通常不使用核函数；svm自带正则话，因此是结构风险最小化算法。68 特征选择的方法。69 AUC是什么？就是ROC曲线下的部分，表示什么？70 算法题：快速排序。71 算法题： 一个数组中超过一半的数字。72 有一个能产生1-5的随机数的函数，怎么修改之后能够产生1-7的随机数。73 快速排序，归并排序，深度遍历和广度遍历。74 解释一下lucene原理，怎么进行中文分词，基于什么进行分词。75 算法题：a-z所有字母组合方式。76 算法题：输入一个数，输出这个整数里面最大的质数。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>面试问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主题模型]]></title>
    <url>%2F2019%2F05%2F07%2F%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[主题模型产生的背景传统的信息检索（Information Retrieve）系统中通常使用文档相似性来判别查询（Query）与海量文本的相似性来得到两者之间的关联程度。然而，这种方法只能从文字表面得出query与查询文本之间的相似度，不能够表征两者之间语义上的关联。因此，基于这种文档相似度的方法的信息检索的返回字面上高度相似但语义不相关联的结果。为了解决这一问题，能够表征语义关联的主题模型就被提出来了。在自然语言理解的任务中，可以从文本的不同粒度（字符级、词级、句子级、段级和文档级）提取语义。主题模型就是一种能够从文档级表征文本语义的模型。主题建模就是在文档集合中学习、识别和提取主题的过程。主题模型基于两个相同的假设：（1）每个文档包含多个主题；（2）每个主题含有多个单词。实际上，文档的语义由一些隐变量（或潜在变量）管理。主题模型的目标就是揭示这些潜在变量，即主题。主题塑造了文档和语料库的含义。本文介绍常用的主题模型：LSA（Latent Semantic Analysis），pLSA(probability Latent Semantic Analysis)，LDA（Latent Dirichlet Allocation）和LDA2Vec。 LSA：潜在语义分析LSA的核心思想是把我们所拥有的文档-术语矩阵分解成相互独立的文档-主题和主题-术语矩阵。对于一个给定的语料库，假设该语料库中有$m$个文档，每个文档有$n$个单词。于是我们可以由此构造规模为$m\times n$的文档-术语矩阵$A$，其中行表示一篇文档，列表示该文档存在的单词。$A$中的元素$a_{ij}$可以由两种方式来确定：（1）$a_{ij}$是$j$个词在$i$篇文档中的频度；（2）$a_{ij}$用TF-IDF（Term Frequency–Inverse Document Frequency，词频-逆文本频率指数）来表示。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。由上述方式构造出来的文档-术语矩阵是一个规模庞大但稀疏的矩阵，维度存在冗余。为了找出能够捕捉单词和文档关系的少数潜在主题，有必要降低$A$的维度。我们使用SVD（Singular Value Decomposition，奇异值分解）截断来对$A$进行降维。SVD会将$A$分解成三个独立的矩阵的乘积，即$A = U \Lambda V$。其中$Lambda$是$A$的对角阵，称为奇异值，$U$称为文档-主题矩阵，$V$称为术语-主题矩阵。截断SVD的降维方式是：选择奇异值$\Lambda$中最大的$t$个数，且只保留矩阵$U$和$V$中的前$t$列。其中$t$是一个可以根据主题数量进行选择和调整的超参数。我们以数据集20_newsgroup为例来简单体会一下LSA用来建模主题。在正式引入源码之前先介绍一下20_newsgroup数据集。20 newsgroups数据集18000篇新闻文章，一共涉及到20种话题，所以称作20 newsgroups text dataset，分为两部分：训练集和测试集，通常用来做文本分类。20种话题如下： alt.atheism comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space soc.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc LSA源码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import numpy as npimport umapimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom nltk.corpus import stopwordsfrom sklearn.datasets import fetch_20newsgroupsfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.decomposition import TruncatedSVDpd.set_option("display.max_colwidth", 200)dataset = fetch_20newsgroups(data_home="/data/cgfth/JuNB/TopicModel", shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))documents = dataset.datanews_df = pd.DataFrame(&#123;'document':documents&#125;)news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z#]", " ")news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)&gt;3]))news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())stop_words = stopwords.words('english')tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])detokenized_doc = []for i in range(len(news_df)): t = ' '.join(tokenized_doc[i]) detokenized_doc.append(t)news_df['clean_doc'] = detokenized_doc vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True)X = vectorizer.fit_transform(news_df['clean_doc'])svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)svd_model.fit(X)# 显示与话题相关的一些术语terms = vectorizer.get_feature_names()for i, comp in enumerate(svd_model.components_): terms_comp = zip(terms, comp) sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7] print("Topic "+str(i)+": ") for t in sorted_terms: print(t[0]) print(" ")# 可视化, 如图1X_topics = svd_model.fit_transform(X)embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)plt.figure(figsize=(7,5))plt.scatter(embedding[:, 0], embedding[:, 1], c = dataset.target, s = 10, # size edgecolor='none' )plt.show() LSA主题模型可视化LSA是以共现表（文档-术语矩阵）的奇异值分解形式实现的，是用于小规模数据集，可用于文档分类/聚类、同义词/多义词检索、跨语言检索。SVD的计算是一个十分耗时的过程，并且潜在语义数量$t$的选择对结果的影响很大。此外，LSA的原理简单但得到的不是概率模型，缺乏统计基础，矩阵种的负值难以解释，难以对应到现实中的概念。 pLSA: 概率潜在语义分析pLSA是基于派生自LCM（least common multiple，最小公倍数）的混合矩阵分解，属于概率图模型（probabilistic graphical model，概率图模型是用图来表示变量概率依赖关系的理论，结合概率论与图论的知识，利用图来表示与模型有关的变量的联合概率分布）中的生成模型。生成式模型的代表是：一元模型（Unigram Model）、混合一元模型、n元语法模型、隐马尔可夫模型、朴素贝叶斯模型等。判别式模型的代表是：最大熵模型、支持向量机、条件随机场、感知机模型等。多项式分布：若伯努利分布由单变量扩展到$d$维的向量$x_i$，$x_i={0,1}$且$\sum_{i = 1}^{d}x_i = 1$。进一步，假设$x_i$取$1$得概率为$\mu_i \in \sum_{i = 1}^{d} \mu_i = 1$。则：\begin{equation}\begin{split}P(x|\mu) &amp;= \Pi_{i = 1}^{d} \mu_i^{x_i} \\\mathbb{E}[X_i] &amp;= \mu_i \\D[X_i] &amp;= \mu_i(1-\mu_i)\end{split}\tag{1}\end{equation}将二项分布扩展到多项分布（multinomial distribution）。多项分布描述在$N$次独立实验中有$m_i$次$x_i = 1$的概率。\begin{equation}\begin{split}P(m_1,\cdots,m_d| N, \mu) = \frac{N!}{m_1!\cdots m_d!}\Pi_{i=1}^{d}\mu_i^{m_i}\end{split}\tag{2}\end{equation}一元模型假设每篇文档中的词出现的次数服从多项式分布（式2）。 LDA： 潜在狄利克雷分布LDA2Vec]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>主题建模</tag>
        <tag>自然语言理解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OOV问题和BPE算法]]></title>
    <url>%2F2019%2F05%2F06%2FOOV%E9%97%AE%E9%A2%98%E5%92%8CBPE%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[OOV与BPE简述自然语言处理（NLP）的许多相关任务如实体关系抽取、问答，机器翻译、阅读理解、文本摘要、实体链接等都需要对语言建模。近几年常用的语言模型有词向量，如Word2vec，Glove等。面向不同的领域和不同的任务，研究人员根据实际需要训练出来了各种各样的语言模型，如对实体链接训练了实体嵌入，对知识图谱的表示训练了知识图谱嵌入（实际上是一个三元组的向量化表示）。这些语言建模促使不同的NLP任务取得不同程度的进步。然而，词向量是一个词的有限集合的向量化表示，这些词都是常见词。故而由此而建成的语言模型（Language Mode，LM）总会存在不能满足实际应用的需求，即：目标任务中可能出现一些罕见词或是派生词，词的复数或者其他的一些组合词的规则而产生的词无法用现有词向量模型表示。所以这个问题就称之为OOV(Out-Of-Vocabulary)问题。为了解决这个问题，Rico Sennrich等人提出了BPE（Byte Pair Encoder）算法， 也叫做digram coding双字母组合编码，主要目的是为了数据压缩。算法描述为字符串里频率最常见的一对字符被一个没有在这个字符中出现的字符代替的层层迭代过程。利用BPE算法旨在发现各种介于word和character之间的词根，从而尽可能的覆盖各种各样的OOV。 BPE算法示例12345678910111213141516比如我们想编码：aaabdaaabac我们会发现这里的aa出现的词数最高（我们这里只看两个字符的频率），那么用这里没有的字符Z来替代aa：ZabdZabacZ=aa此时，又发现ab出现的频率最高，那么同样的，Y来代替ab：ZYdZYacY=abZ=aa同样的，ZY出现的频率大，我们用X来替代ZY：XdXacX=ZYY=abZ=aa最后，连续两个字符的频率都为1了，也就结束了。解码的时候，就按照相反的顺序更新替换即可。 BPE算法代码1234567891011121314151617181920212223242526import re, collectionsdef get_stats(vocab): pairs = collections.defaultdict(int) for word, freq in vocab.items(): symbols = word.split() for i in range(len(symbols)-1): pairs[symbols[i], symbols[i+1]] += freq return pairsdef merge_vocab(pair, v_in): v_out = &#123;&#125; bigram = re.escape(' '.join(pair)) p = re.compile(r'(?&lt;!\S)' + bigram + r'(?!\S)') for word in v_in: w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] return v_outvocab = &#123;'l o w &lt;/w&gt;' : 5, 'l o w e r &lt;/w&gt;' : 2, 'n e w e s t &lt;/w&gt;':6, 'w i d e s t &lt;/w&gt;':3&#125;num_merges = 10for i in range(num_merges): pairs = get_stats(vocab) best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) print(best) 输出为：12345678910('e', 's')('es', 't')('est', '&lt;/w&gt;')('l', 'o')('lo', 'w')('n', 'e')('ne', 'w')('new', 'est&lt;/w&gt;')('low', '&lt;/w&gt;')('w', 'i')]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机]]></title>
    <url>%2F2019%2F05%2F05%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分治算法]]></title>
    <url>%2F2019%2F04%2F30%2F%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>数据结构与算法分析</category>
      </categories>
      <tags>
        <tag>分而治之</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BP神经网络]]></title>
    <url>%2F2019%2F04%2F29%2FBP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[初识BPBP（Back Propagation）神经网络是整个深度学习的基础。当前，大多数深度神经网络仍然采用BP作为误差反向传播的技术。通常，BP是指具有三层网络结构的浅层神经网络，如图1。实际上，BP除了输入层和输出层外，只有1层隐藏层，所以它是浅层神经网络。 图1 BP神经网络在正式推导BP的数学表示之前，我们需要先明确一下相关数学符号的意义。如图1，我们记BP的网络层数为$n_l$，则$n_l=3$。记BP网络的第$l(l=1,2,3)$层为$L_l$。故图1中的输入层、隐层和输出层分别记为$L_1$、$L_2$和$L_3$。 记$L_l$的第$i$个神经元与$L_{l+1}$的第$j$个神经元连接的权重为$W_{ij}^{(l)}$。图1中的$1$是一个偏置项，记$L_l$的偏置项为$b^{(l)}$。 BP前向传播设$L_l$的第$i$个神经元的输入为$z_i^{(l)}$，$L_l$的第$i$神经元的输出为$a_i^{(l)}$。显然，当$l=1$时，$a_i^{l}=x_i$。假设所使用的激活函数记为$f(x)$。于是，BP的前向传播可如下式计算：隐层：\begin{equation}\begin{split}z_1^{(2)} &amp;= W_{11}^{(1)}x_1 + W_{21}^{(1)}x_2 + \cdots + W_{n1}^{(1)}x_n + b^{(1)} \\a_{1}^{(2)} &amp;= f(z_1^{(2)}) = f(W_{11}^{(1)}x_1 + W_{21}^{(1)}x_2 + \cdots + W_{n1}^{(1)}x_n + b^{(1)}) \\z_2^{(2)} &amp;= W_{12}^{(1)}x_1 + W_{22}^{(1)}x_2 + \cdots + W_{n2}^{(1)}x_n + b^{(1)} \\a_{2}^{(2)} &amp; = f(z_2^{(2)}) = f(W_{12}^{(1)}x_1 + W_{22}^{(1)}x_2 + \cdots + W_{n2}^{(1)}x_n + b^{(1)}) \\\vdots \\z_m^{(2)} &amp;= W_{1m}^{(1)}x_1 + W_{2m}^{(1)}x_2 + \cdots + W_{nm}^{(1)}x_n + b^{(1)} \\a_m^{(2)} &amp;= f(z_m^{(2)}) = f(W_{1m}^{(1)}x_1 + W_{2m}^{(1)}x_2 + \cdots + W_{nm}^{(1)}x_n + b^{(1)})\end{split}\tag{1}\end{equation}输出层：\begin{equation}\begin{split}z_1^{(3)} &amp;= W_{11}^{2}a_{1}^{(2)} + W_{21}^{(2)}a_{2}^{(2)} + \cdots + W_{m1}^{(2)}a_{m}^{(2)} + b^{(2)} \\a_{1}^{(3)} &amp;= f(z_1^{(3)}) = f(W_{11}^{2}a_{1}^{(2)} + W_{21}^{(2)}a_{2}^{(2)} + \cdots + W_{m1}^{(2)}a_{m}^{(2)} + b^{(2)}) \\z_2^{(3)} &amp;= W_{12}^{2}a_{1}^{(2)} + W_{22}^{(2)}a_{2}^{(2)} + \cdots + W_{m2}^{(2)}a_{m}^{(2)} + b^{(2)} \\a_{2}^{(3)} &amp;= f(z_2^{(3)}) = f(W_{12}^{2}a_{1}^{(2)} + W_{22}^{(2)}a_{2}^{(2)} + \cdots + W_{m2}^{(2)}a_{m}^{(2)} + b^{(2)}) \\\vdots \\z_p^{(3)} &amp;= W_{1p}^{2}a_{1}^{(2)} + W_{2p}^{(2)}a_{2}^{(2)} + \cdots + W_{mp}^{(2)}a_{m}^{(2)} + b^{(2)} \\a_p^{(3)} &amp;= f(z_p^{(3)}) = f(W_{1p}^{2}a_{1}^{(2)} + W_{2p}^{(2)}a_{2}^{(2)} + \cdots + W_{mp}^{(2)}a_{m}^{(2)} + b^{(2)})\end{split}\tag{2}\end{equation}我们将上面的输出层的式子用统一的形式表述之。即\begin{equation}\hat{y} = f(\sum_{i = 1}^{m}w_{ij}^{(2)}a_i^{(2)} + b^{(2)}), j = 1, 2, \cdots, p\tag{3}\end{equation} 反向传播设有$s$个训练样本${(X^{(1)}, y^{(1)}), (X^{(2)}, y^{(2)}), \cdots, (X^{(s)}, y^{(s)})}$。对于训练样本$(X, y)$，其损失函数为：\begin{equation}L(W, b) = \frac{1}{2} \left|| \hat{y}- y \right||^2\tag{4}\end{equation}通常，为了防止模型过拟合，损失函数会加上正则项$R$。即\begin{equation}L(W, b) = L(W, b) + R\tag{5}\end{equation}其中$R$为：\begin{equation}R = \frac{\lambda}{2}\sum_{l=1}^{n_l - 1}\sum_{i = 1}^{q_l}\sum_{j = 1}^{q_{l + 1}}(W_{ij}^{(l)})\tag{6}\end{equation}这里的$q_l$表示第$L_l$上的神经元数量。所以最终的损失函数$L$可形式化为：\begin{equation}L(w, b) = \frac{1}{s}\sum_{i = 1}^{s}L(W, b) + \frac{\lambda}{2}\sum_{l=1}^{n_l - 1}\sum_{i = 1}^{q_l}\sum_{j = 1}^{q_{l + 1}}(W_{ij}^{(l)})\tag{7}\end{equation}其中$\lambda$为正则化率。 为使损失函数$L(w, b)$得到最小值，我们需要通过梯度下降来调整权重$W$以及偏置项$b$。其数学表述如下：\begin{equation}\begin{split}W_{ij}^{(l)} &amp;= W_{ij}^{(l)} -\alpha \frac{\partial}{\partial W_{ij}^{(l)}} L(W, b) \\b_{i}^{(l)} &amp;= b_{i}^{(l)} - \alpha \frac{\partial}{\partial b_{i}^{(l)}} L(W, b)\end{split}\tag{8}\end{equation}其中$\alpha$称为学习率。我们对$(7)$式分别对$W$和$b$求偏导：\begin{equation}\begin{split}\frac{\partial}{\partial W_{ij}^{(l)}}L(W, b) &amp;= \frac{1}{s}\sum_{i = 1}^{s}\frac{\partial}{\partial W_{ij}^{(l)}}L(W, b) + \lambda W_{ij}^{(l)} \\\frac{\partial}{\partial b_i^{(l)}}L(W, b) &amp;= \frac{1}{s}\sum_{i = 1}^{s}\frac{\partial}{\partial b_i^{(l)}}L(W, b)\end{split}\tag{9}\end{equation} BP源码BP源码]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节跳动2018校招算法方向编程题第3题]]></title>
    <url>%2F2019%2F04%2F29%2F%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A82018%E6%A0%A1%E6%8B%9B%E7%AE%97%E6%B3%95%E6%96%B9%E5%90%91%E7%BC%96%E7%A8%8B%E9%A2%98%E7%AC%AC3%E9%A2%98%2F</url>
    <content type="text"><![CDATA[题目描述产品经理(PM)有很多好的idea，而这些idea需要程序员实现。现在有$N$个PM，在某个时间会想出一个idea，每个idea有提出时间、所需时间和优先等级。对于一个PM来说，最想实现的idea首先考虑优先等级高的，相同的情况下优先考虑所需时间最小的，还相同的情况下选择最早想出的，没有 PM会在同一时刻提出两个idea。 同时有$M$个程序员，每个程序员空闲的时候就会查看每个PM尚未执行并且最想完成的一个idea,然后从中挑选出所需时间最小的一个idea独立实现，如果所需时间相同则选择PM序号最小的。直到完成了idea才会重复上述操作。如果有多个同时处于空闲状态的程序员，那么他们会依次进行查看idea的操作。 求每个idea实现的时间。 输入第一行三个数$N$、$M$、$P$，分别表示有$N$个PM，$M$个程序员，$P$个idea。随后有$P$行，每行有$4$个数字，分别是PM序号、提出时间、优先等级和所需时间。输出$P$行，分别表示每个idea实现的时间点。 输入描述输入第一行三个数$N$、$M$、$P$，分别表示有$N$个PM，$M$个程序员，$P$个idea。随后有$P$行，每行有$4$个数字，分别是PM序号、提出时间、优先等级和所需时间。全部数据范围$[1, 3000]$。 输出描述输出$P$行，分别表示每个idea实现的时间点。 输入示例2 2 51 1 1 21 2 1 11 3 2 22 1 1 22 3 5 5 输出示例34539 示例解析这是一个多执行机多任务调度问题。我们记PM序号、提出时间、优先等级和所需时间相对于程序员的优先级分别记为$P_{PMSeqNum}$、$P_{PropTime}$、$P_{Pror}$和$P_{RunTime}$。显然： $$P_{Prior} &gt; P_{RunTime} &gt; P_{PMSeqNum} &gt; P_{PropTime} $$ 注意：这里的优先等级表示数字越小，则优先级越高。我们以一张表格来表示程序员实现idea的整个过程。 01 2 34 5 67 8 9 1 (1, 1, 2, 1, 1) 2 (1, 1, 1, 1, 2) 3 (2, 2, 2, 1, 2) 4 (2, 1, 2, 2, 1) 5 (1, 5, 5, 2, 3) 说明： 这里的五元组$(ID, Prior, RunTime, PMSeqNum, PropTime)$分别表示程序员的ID，idea优先等级、 运行时间、 PM序号和提出时间。绿色表示idea处于执行态，褐色表示idea处于等待状态。 整体思路重点代码讲解完整代码]]></content>
      <categories>
        <category>笔试编程</category>
      </categories>
      <tags>
        <tag>任务调度</tag>
        <tag>优先级队列</tag>
        <tag>堆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔试题]]></title>
    <url>%2F2019%2F04%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、 一个二进制$X$发出符号集为${-1, 1}$，经过离散无记忆信道传输，由于信道中噪音的存在，在接收端$Y$收到符号集为${-1, 1, 0}$。已知$P(x=-1)=\frac{1}{4}$，$P(x=1)=\frac{3}{4}$，$P(y=-1|x=1)=\frac{4}{5}$，$P(y=0|x=-1)=\frac{1}{5}$，$P(y=1|x=1)=\frac{3}{4}$，$P(y=0|x=1)=\frac{1}{4}$，求条件熵$H(Y|X)$ () A 0.2375B 0.3275C 0.5273D 0.5372 解析:\begin{split}\because\quad H(X|Y) &amp;=-\sum P(X, Y)\log P(Y|X) \\ &amp;= -\sum P(Y|X)P(X)\log P(Y|X)\end{split} \begin{split} \therefore\quad H(X|Y)&amp;=-(P(y=-1|x=-1)P(x=-1)\log P(y=-1|x=-1) \\ &amp; + P(y=0|x=-1)P(x=-1)\log P(y=0|x=-1) \\ &amp; + P(y=1|x=1)P(x=1)\log P(y=1|x=1) \\ &amp; + P(y=0|x=1)P(x=1)\log P(y=0|x=1)) \\ &amp; =-(\frac{4}{5}\times\frac{1}{4}\times\log\frac{4}{5} + \frac{1}{5}\times\frac{1}{4}\times\log\frac{1}{5} + \frac{3}{4}\times\frac{3}{4}\times\log\frac{3}{4} + \\ &amp; \frac{1}{4}\times\frac{3}{4}\times\log\frac{1}{4}) \\&amp;\approx -(-0.01938-0.03495-0.07028-0.11289)=0.2375\end{split} 特别注意： 这里的$log$是以$10$为底的对数函数。答案：A2、 下列哪个不属于CRF模型对于HMM和MEMM模型的优势A 特征灵活B 速度快C 可容纳较多上下文信息D 全局最优解析：HMM（Hidden Markov Model, 隐马尔可夫模型）是对转移概率和表现概率直接建模， 统计共现概率。MEMM（Maximum Entropy Markov Model, 最大熵马尔可夫模型）是对转移概率和发射概率建立联合概率，统计时统计的时条件概率， 但由于MEMM只在局部做归一化，故容易陷入局部最优。CRF(Conditional Random Field，条件随机场)统计了全局概率。再做归一化时考虑了数据在全局的分布，而不是仅仅只做局部归一化，这样解决了MEMM中的标记偏置（Label Bias）的问题。CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。转移概率：给定的马氏链在某时刻处于某一状态，再经若干时间达到另一状态的条件概率。表现概率：观察变量的条件概率，亦称发射概率。共现概率：答案：B3、 模式识别中，不属于马氏距离较之于欧式距离的优点是（）A 平移不变性B 尺度不变性C 考虑了模式的分布解析：马氏距离（Mahalanobis Distance）是由印度统计学家马哈拉洛比斯提出的，表示数据的协方差距离，是一种有效的计算两个未知样本集相似度的方法。与欧式距离不同的是，马氏距离考虑到各种特性之间的关系并且是尺度无关的（scale-invariant），即独立于测量尺度。对于一个均值为$\mu=(\mu_1, \mu_2, \cdots, \mu_p)^T$, 协方差矩阵为$\Sigma$的多变量向量为$x=(x_1, x_2, \cdots, x_p)^T$, 其马氏距离为：\begin{equation}D_M(x) =\sqrt{(x-\mu)^T\Sigma^{-1}(x-\mu)}\end{equation}对于两个变量$x$和$y$， 其协方差计算公式：\begin{equation}cov(x, y) = \mathbb{E}(x-\mathbb{E}(x))(y-\mathbb{E}(y))\end{equation}对于含有$n$个变量（或属性）($c_1, c_2, \cdots, c_n$)的数据集，则可得协方差矩阵：\begin{equation}\Sigma=\begin{bmatrix}cov(c_1, c_1) &amp; cov(c_1, c_2) &amp; \cdots &amp; cov(c_1, c_n) \\cov(c_2, c_1) &amp; cov(c_2, c_2) &amp; \cdots &amp; cov(c_2, c_n) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\cov(c_n, c_1) &amp; cov(c_n, c_2) &amp; \cdots &amp; cov(c_n, c_n)\end{bmatrix}\end{equation}如果协方差矩阵为单位阵，则马氏距离就是欧式距离；如果协方差矩阵为对角阵，此时称马氏距离为正规化的欧式距离。马氏距离的优点：马氏距离不受量纲影响：两点之间的马氏距离与原始数据的测量单位无关；由标准化数据和中心化数据计算的二点之间的距离相同；马氏距离可以排除变量之间的相关性的干扰。欧式距离特性有：平移不变性、旋转不变性。马式距离特性有：平移不变性、旋转不变性、尺度缩放不变性、不受量纲影响的特性、考虑了模式的分布。 答案：A 4、 以下（）不属于线性分类器最佳准则。A 感知器准则函数B 贝叶斯分类C 支持向量机D Fisher准则 解析：线性分类器有三大类：感知器准则函数、SVM、Fisher准则。贝叶斯分类器不是线性分类器。感知器准则函数：准则函数以使错分类样本到分界面距离之和最小为原则。优点是通过错分类样本提供的信息对分类器函数进行修正。支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小（使用核函数可解决非线性问题）。Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条原点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。 答案：B 5、 下面有关分类算法的准确率、召回率和$F1$值的描述，错误的是：A 准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率B 召回率是指检索出相关文档数和文档库中所有相关文档数的比率，衡量的是检索系统的查全率C 正确率、召回率和$F1$值取值都在$0$和$1$之间，数值越接近$0$，查准率和查全率越高D 为了解决准确率和召回率冲突的问题，引入了$F1$分数 解析：这里我们使用文档分类作为示例来阐述准确率、精准率、召回率、$F1$和$AUC$。假设文档库含有$n$篇文档，这些文档可分为$k$类，每类文档具有$m_k$篇文档。设有某算法可对该文档库进行分类，每类正确归档数记为$c_k(c_k &lt;= m_k)$。准确率（Accuracy）即是该算法分类正确的文档占所有文档库的比率。\begin{split}acc = \frac{\sum_{i = 1}^{k}c_k}{\sum_{i = 1}^{k}m_k}\times 100\% = \frac{\sum_{i = 1}^{k}c_k}{n} \times 100\%\end{split}其中$n=\sum_{i = 1}^{k}m_k$。在介绍其他概念之前，需要引入基于二分类的混淆矩阵。实际上，多分类问题可以划分为多个二分类问题。具体地，在多分类问题中，对某一类别$k_i(i=1,2,\cdots,k)$, 若该算法得到的类别标签属于该类别，则分类正确；若由算法得到的分类标签属于其他类别，则分类错误。由此得到了$k_i$类和其他类的二分类问题。 混淆矩阵定义如下： 预测值0 预测值1 真实值0 TN FP 真实值1 FN TP 我们将当前所关注的分类类别的数据项称之为正例，而不受当前关注的类别的数据项称之为负例。则有：TN：算法预测为负例（N），实际上也是负例（N）的个数，即算法预测对了（True）；​FP：算法预测为正例（P），实际上是负例（N）的个数，即算法预测错了（False）；​FN：算法预测为负例（N），实际上是正例（P）的个数，即算法预测错了（False）；​TP：算法预测为正例（P），实际上也是正例（P）的个数，即算法预测对了（True）。于是精准率(Precision)、召回率(Recall)和$F1$值定义如下：\begin{equation}prec = \frac{TP}{TP+FP}\end{equation} \begin{equation}recall = \frac{TP}{TP+FN}\end{equation} \begin{equation}F1 = \frac{2\times prec \times recall}{prec + recall}\end{equation} AUC被定义为ROC曲线下的面积。诸如逻辑回归这样的分类算法而言，通常预测的都是一个概率值，我们会认为设置一个阈值，超过这个阈值，就预测为其中一类，不超过这个阈值，定义为另外一类。于是，不同的阈值就对应了不同的假正率和真正率，于是通过不同的阈值就形成了假正率和真正率序列，它们就可以在直角坐标系上通过描点成为光滑曲线，这个曲线就是 ROC 曲线。横坐标：假正率（False positive rate， FPR），预测为正但实际为负的样本占所有负例样本的比例。\begin{equation}FPR=\frac{FP}{TN+FP}\end{equation}纵坐标：真正率（True positive rate， TPR），这个其实就是召回率，预测为正且实际为正的样本占所有正例样本的比例。\begin{equation}TPR=\frac{TP}{TP+FN}\end{equation} 答案：C 6、 一下几种模型方法属于判别式模型（Discriminative）的有（）（1）混合高斯模型（2）条件随机场模型（3）区分度模型（4）隐马尔可夫模型A 2,3B 3,4C 1,4D 1,2解析:常见的判别式模型有：Logistic Regression（LR，逻辑回归）Linear Discriminative Analysis(LDA，线性判别分析, Fisher准则)Support Vector Machine(SVM，支持向量机)Boosting（集成学习）Conditional Random Field（CDF, 条件随机场）Linear Regression（LR, 线性回归）Neural Networks(NN, 神经网络)常见的生成模型：Gaussian Mixture Model（GMM，高斯混合模型以及其他类型的混合模型）Hidden Markov Model（HMM，隐马尔可夫模型）Naive Bayies（朴素贝叶斯）Averaged One-Dependence Estimators （AODE，平均单依赖估计）Latent Dirichlet Allocation（LDA主题模型）Restricted Boltzmann Machine(RBM，受限玻尔兹曼机)答案：A 7、你正在使用带有L1正则化的logistic回归做二分类，其中$C$是正则化参数，$w_1$和$w_2$是$x_1$和$x_2$的系数。当你把$C$值从$0$增加到非常大的值时，下面哪个选项是正确的？A 第一个$w_2$成了$0$，接着$w_1$也成了$0$B 第一个$w_1$成了$0$，接着$w_2$也成了$0$C $w_1$和$w_2$同时成了$0$D 即使在$C$成为最大值之后，$w_1$和$w_2$都不能成为$0$解析：通过图1可以看出$w_1$和$w_2$同时成了$0$ import numpy as np import numpy as np from matplotlib import pyplot as plt from mpl_toolkits.mplot3d import Axes3D fig = plt.figure() ax1 = Axes3D(fig) L1 = 1 W1 = np.linspace(-3, 3, 60) W2 = np.linspace(-5, 5, 60) C = L1 / (abs(W1) + abs(W2)) ax1.scatter3D(W1, W2, C) ax1.plot3D(W1, W2, C, 'red') ax1.set_title("$L1=C*(|w1|+|w2|)$") ax1.set_xlabel("$w_1$") ax1.set_ylabel("$w_2$") ax1.set_zlabel("$C$") plt.show() 图1 L1正则化函数图像答案：C 8、下列哪个不属于常用的文本分类的特征选择算法？A $\chi^2$检验值B 互信息C 信息增益D 主成分分析解析：常用的特征选择方法：（1）DF（Document Frequency，文档频率）：统计特征词出现的文档数量，用来衡量特征词的重要性；（2）MI（Mutual Information，互信息法）：用于衡量特征词与文档类关系的信息量。如果某个特征词的频率很低，那么互信息得分就会很高。反之，如果该特征词的频率很高，则互信息得分会很低。因此互信息法倾向低频的特征词；（3）IG（Information Gain，信息增益法）：在某个特征词的缺失与存在的两种情况，根据语料中前后信息的变化来衡量特征词的重要性。（4）$\chi^2$检验法：利用统计学中“假设检验”的基本思想。首先假设特征词与类别不直接相关，如果利用$\chi^2$分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备择假设：特征词与类别有着很高的关联度。（5）WLLR（Weighed Log Likelihood Ration，加权对数似然比）；（6）WFO（Weighted Frequency and Odds）:加权频率和可能性。答案： D 9、下列不是SVM核函数的是（）A 多项式和函数B logistic核函数C 径向基核函数D Sigmoid核函数 解析：SVM核函数包括线性核函数、多项式核函数、径向基(RBF)核函数（Guassian核函数）、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。根据泛函有关理论，只要一种函数$\kappa(x_i, x_j)$满足Mercer条件，它就对应某一种变换空间的内积。Mercer定理：任何半正定的函数都可以作为核函数。所谓半正定的函数$f(x_i, x_j)$，是指对训练数据的集合$(x_1, x_2, \cdots, x_n)$定义一个由元素$a_{ij}=f(x_i, x_j)$组成的矩阵。显然，这个矩阵的规模是$n \times n$。如果该矩阵是半正定的，则$f(x_i, x_j)$就称为半正定函数。常见的核函数：（1）线性核函数\begin{equation}\kappa(x, z) = &lt;x, z&gt;\end{equation}（2）多项式核函数\begin{equation}\kappa(x, z) = (&lt;x, z&gt; + 1)^d, r\in Z^+\end{equation}（3）RBF核函数\begin{equation}\kappa(x, z) = e^{-\frac{\left||x-z\right||^2}{2\sigma^2}}, \sigma=R-\{0\}\end{equation}（4）幂指数核函数\begin{equation}\kappa(x, z) = e^{-\frac{\left||x - z\right||}{2\sigma^2}}\end{equation}（5）拉普拉斯核函数\begin{equation}\kappa(x,z) = e^{-\frac{\left||x - z \right||}{\sigma}}\end{equation}（6）ANOVA核函数\begin{equation}\kappa(x, z) = \sum_{k = 1}^n e\left(\sigma(x^k - z^k)^2 \right) ^ d\end{equation}（7）二次有理核函数\begin{equation}\kappa(x, z) = 1 - \frac{\left|| x - z \right||^2}{\left||x - z \right||^2 + c}\end{equation}（8）多元二次核函数\begin{equation}\kappa(x, z) = \sqrt{\left|| x - z \right||^2 + c^2}\end{equation}（9）逆多元二次核函数\begin{equation}\kappa(x, z) = \frac{1}{\sqrt{\left|| x - z \right||^2 + c^2}}\end{equation}（10）sigmoid核函数\begin{equation}\kappa(x, z) = \tanh(\alpha x^T + c)\end{equation}（11）傅立叶核函数\begin{equation}\kappa(x_i, z_j) = \frac{1}{2} + \sum_{k=1}^{N}(\sin(kx_i)\sin(kz_j) + \cos(kx_i)\cos(kz_j))\end{equation} 注：$d$表示阶数。$d$越大，映射的维度越高，计算量越大，此时容易出现“过拟合现象”。采用sigmoid作为核函数时，支持向量机实际就是一种多层感知器神经网络。此时，神经网络的隐含层节点数目、隐含层节点对输入节点的权值都是训练过程中自动确定的。支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最优值，也保证了它对于未知样本的良好泛化性能而不会出现“过拟合”现象。核函数的选择：在选择核函数解决实际问题时，通常采用的方法有：（1）利用专家的先验知识预先选定核函数；（2）采用交叉验证（Cross-validation）方法。即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数。如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多。（3）采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想。答案：B]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[math]]></title>
    <url>%2F2019%2F04%2F27%2Fmath%2F</url>
    <content type="text"><![CDATA[$\frac{x}{y}$]]></content>
  </entry>
  <entry>
    <title><![CDATA[强化学习]]></title>
    <url>%2F2019%2F04%2F27%2F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[\begin{equation}f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu^)2}{z\sigma^2}}\end{equation}]]></content>
  </entry>
  <entry>
    <title><![CDATA[NLP]]></title>
    <url>%2F2019%2F04%2F27%2FNLP%2F</url>
    <content type="text"><![CDATA[1、 一个二进制$X$发出符号集为${-1, 1}$，经过离散无记忆信道传输，由于信道中噪音的存在，在接收端$Y$收到符号集为${-1, 1, 0}$。已知$P(x=-1)=\frac{1}{4}$，$P(x=1)=\frac{3}{4}$，$P(y=-1|x=1)=\frac{4}{5}$，$P(y=0|x=-1)=\frac{1}{5}$，$P(y=1|x=1)=\frac{3}{4}$，$P(y=0|x=1)=\frac{1}{4}$，求条件熵$H(Y|X)$ () A 0.2375B 0.3275C 0.5273D 0.5372 解析:\begin{split}\because\quad H(X|Y) &amp;=-\sum P(X, Y)\log P(Y|X) \\ &amp;= -\sum P(Y|X)P(X)\log P(Y|X)\end{split} \begin{split} \therefore\quad H(X|Y)&amp;=-(P(y=-1|x=-1)P(x=-1)\log P(y=-1|x=-1) \\ &amp; + P(y=0|x=-1)P(x=-1)\log P(y=0|x=-1) \\ &amp; + P(y=1|x=1)P(x=1)\log P(y=1|x=1) \\ &amp; + P(y=0|x=1)P(x=1)\log P(y=0|x=1)) \\ &amp; =-(\frac{4}{5}\times\frac{1}{4}\times\log\frac{4}{5} + \frac{1}{5}\times\frac{1}{4}\times\log\frac{1}{5} + \frac{3}{4}\times\frac{3}{4}\times\log\frac{3}{4} + \\ &amp;\frac{1}{4}\times\frac{3}{4}\times\log\frac{1}{4}) \\&amp;\approx -(-0.01938-0.03495-0.07028-0.11289)=0.2375\end{split} 特别注意： 这里的$log$是以$10$为底的对数函数。答案：A]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识图谱]]></title>
    <url>%2F2019%2F04%2F27%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[随机森林]]></title>
    <url>%2F2019%2F04%2F27%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[随机森林（Random Forest, RF）算法是一种重要的基于Bagging的集成学习方法。随机森林可以用来解决分类、回归等问题。在正式介绍随机森林之前，需要介绍集成学习的相关概念。 集成学习集成学习的思想就是要通过训练多个分类器来共同解决一个复杂分类问题。在集成学习方法中，其泛化能力比单个学习算法的泛化能力强很多。根据多个分类器学习方式的不同，集成学习方法可以分为Bagging算法和Boosting算法。（1）Bagging（Bootstrap Aggregating）算法通过对训练样本有放回的抽取，由此产生多个训练数据子集，并在每个训练数据子集上训练一个分类器。最终的分类结果由多个分类器投票产生。（2）Boosting算法通过顺序地给训练集中的数据项重新加权创造创造不同的基础学习器。训练开始时，所有的数据项都被初始化为同一个权重。之后，每次增强的迭代都会生成一个适应加权之后的训练数据集的基础学习器。每一次迭代的错误率都会计算出来，正确划分的数据项的权重会被降低，而被错误划分的数据项权重将会增大。Boosting算法的最终模型是一系列基础学习器的线性组合，而且系数依赖于各个基础学习器的表现。 随机森林随机森林算法由一系列决策树组成。 通过Bootstrap重采样技术，从原始训练样本集中有放回地重复随机抽样$m$个样本，生成新的训练样本集合，然后根据自助样本集生成$k$个分类树组成随机森林。新数据的分类结果根据决策树投票形成的分数决定。这里我们采用CART分类树作为随机森林的决策树。CART算法是决策树（主要的决策树模型有ID3算法、C4.5算法和CART算法）的一种。于ID3和C4.5不同的是，CART既可以用来解决分类问题，也可以用来处理回归问题。在CART分类树算法中，利用Gini指数作为划分数据集属性的指标。设有一数据集，记为$D$。假设有$K$个分类，样本属于第$k$个类的概率为$p_k$，则此概率的Gini指数为：\begin{equation}Gini(p) = \sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2\tag{1}\end{equation}对于数据集$D$，其Gini指数为：\begin{equation}Gini(D) = 1 - \sum_{k=1}^{K}\left(\frac{|C_k|}{|D|}\right)^2\tag{2}\end{equation}其中$|C_k|$表示数据集$D$中，属于类别$k$的样本个数。若根据特征$A$将数据集$D$划分成独立的两个数据集$D_1$和$D_2$，则此时Gini指数为:\begin{equation}Gini\left(D, A\right) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)\tag{3}\end{equation}在划分数据属性时，需要设置划分终止条件。通常终止条件可以是：（1 节点中的样本数小于给定阈值；（2）样本集的Gini指数小于给定阈值（样本基本属于同一类）；（3）没有更多特征 CART分类树的构建流程如下：（1）遍历训练数据集的所有属性和可能的切分点，寻找最佳切分属性及其最佳切分点，使得切分之后的Gini系数最小；（2）利用找到的最佳属性极其最佳切分点将训练数据集切分成两个子集，分别对应分类树中的左子树和右子树；（3）重复以上步骤（为每一个叶子节点寻找最佳切分属性及其最佳切分点，将其划分为左右字数）知道满足终止条件；（4）生成CART树。 随机森林算法是通过训练多个决策树得到综合的分类模型，其只要两个参数：构建决策树的个数$n_{tree}$和在决策树的每个节点进行分裂时需要考虑的输入特征个数$k$。其中$k$可以取$\log_2n$。这里的$n$表示训练数据集中特征（属性）的个数。对于单棵决策树的构架，流程如下：（1）随机有放回地从训练数据集中抽取$m$个样本；（2）从$n$样本特征中随机挑选$k$个特征，然后从这个$k$个特征中选择最好一个进行分裂（决策树的生成或构建）；（3）重复步骤（2），直到该节点的所有训练样本都属于同一类。注意:在决策树分裂过程中不需要剪支。 补充材料在决策树算法中，划分数据集除了Gini指数外，还有信息增益和增益率。引入信息增益（Information Gain）和增益率（Gain Ratio）之前，需要先介绍熵（Entropy）的概念。熵是度量集合纯度最常用一种指标。信息熵较小表明数据集纯度较高。 对于包含$m$个训练样本的数据集$D:{X^{(1)}, y^{(1)}, \cdots, (X^{(m)}, y^{(m)}}$，在数据集$D$中， 第$k$类样本所占的比例为$p_k$，则数据集$D$的信息熵$En(D)$为：\begin{equation}En(D)=-\sum_{k=1}^{K}\log_2np_k\tag{4}\end{equation}其中$K$表示数据集$D$的类别个数。当把样本按照特征$A$的划分成两个独立的数据集$D_1$和$D_2$时，此时数据集$D$的熵$En(D)$为两个独立数据集$D_1$的熵$En(D_1)$和$En(D_2)$的加权和：\begin{equation}\begin{split}En(D) &amp;= \frac{|D_1|}{|D|}En(D_1) + \frac{|D_2|}{|D|}En(D_2) \\&amp;= -\left( \frac{|D_1|}{|D|}\sum_{k=1}^{K}p_k\log_2p_k + \frac{|D_2|}{|D|}\sum_{k=1}^{K}p_k\log_2p_k \right)\end{split}\tag{5}\end{equation} 信息增益（$igain$）是对划分给定数据集前后信息熵的减少量，即：\begin{equation}igain(D, A) = En(D) - \sum_{p=1}^{P}\frac{|D_p|}{|D|}En(D_p)\tag{6}\end{equation}其中，$D_p$表示属于第$p$类的样本数。在选择数据集划分标准时，通常选择能够使得信息增益最大的划分。ID3算法是利用信息增益作为划分数据集的一种方法。增益率也可以作为选择最优划分属性的方法，C4.5就是采用增益率作为划分数据集的方法。增益率可以基于下式计算:\begin{equation}gain\_ratio(D,A) = \frac{igain(D, A)}{IV(A)}\tag{7}\end{equation}其中$IV(A)$称为$A$的固有值（Intrinsic Value）。即：\begin{equation}IV(A)=\sum_{p=1}^{P}\frac{|D_p|}{|D|}\log_2\frac{|D_p|}{|D|}\tag{8}\end{equation} 源码随机森林源码]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[random-forest]]></title>
    <url>%2F2019%2F04%2F27%2Frandom-forest%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
</search>
