<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[k-means]]></title>
    <url>%2F2019%2F05%2F19%2Fk-means%2F</url>
    <content type="text"><![CDATA[k-means是一种广泛使用的聚类算法。k-means算法是基于相似性的无监督算法，其通过比较样本之间的相似性，将较为相似的样本划分到同一类中。 相似性的度量对于样本$X$和$Y$，要度量其相似性，我们定义距离函数$d(X,Y)$来表示样本$X$和样本$Y$之间的相似性。常用的距离函数有闵可夫斯基距离（Minkowski Distance）、曼哈顿距离（Manhattan Distance）和欧氏距离（Euclidean Distance）。假设有两个点$P$和$Q$，其对应的坐标分别为：\begin{equation}\begin{aligned}P &amp;= (x_1, x_2, \cdots, x_n) \in \mathbb{R}^n \\Q &amp;= (y_1, y_2, \cdots, y_n) \in \mathbb{R}^n\end{aligned}\tag{1}\end{equation}则$P$和$Q$之间的闵可夫斯基距离定义为：\begin{equation}d(P, Q) = \left( \sum_{i = 1}^n \left(x_i - y_i \right)^p \right)^{\frac{1}{p}}\tag{2}\end{equation}$P$和$Q$之间的曼哈顿距离定义为：\begin{equation}d(P, Q) = \sum_{i = 1}^{n} | x_i - y_ i|\tag{3}\end{equation}$P$和$Q$之间的欧氏距离定义为：\begin{equation}d(P, Q) = \sqrt{\sum_{i = 1}^{n} \left( x_i - y_i \right)^2}\tag{4}\end{equation} k-means算法原理k-means是基于数据划分的无监督聚类算法。首先定义聚类类别数$k$，然后随机初始化$k$个聚类中心，通过计算每一个样本与聚类中心之间的相似度，将样本点划分到最相似的类别中。我们假设有$m$个样本$\{ X^{(1)}, X^{(2)}, \cdots, X^{(m)} \}$。其中，$X^{(i)}$表示第$i$个样本，每一个样本中包含$n$个特征$ X^{(i)} =\{ x_1^{(i)}, x_2^{(i)}, \cdots, x_n^{(i)} \} $。首先初始化$k$个聚类中心，通过每个样本与$k$个聚类中心之间的相似度，确定每个样本所属类别，再通过每个类别中的样本重新计算每个类的聚类中心，重复这样的过程，直到聚类中心不再改变，最终确定每个样本所属的类别以及每个类的聚类中心。 k-means算法与矩阵分解假设训练数据集$X$中有$m$个样本$\{ X^{(1)}, X^{(2)}, \cdots, X^{(m)} \}$。其中，每一个样本$X^{(i)}$为$n$维的向量，此时样本可表示成一个$m \times n$的矩阵：\begin{equation}X_{m \times n} = \left( X^{(1)}, X^{(2)}, \cdots, X^{(m)} \right) =\left[\begin{matrix}x_1^{(1)} &amp; x_2^{(2)} &amp; \cdots &amp; x_n^{(1)} \\x_1^{(2)} &amp; x_2^{(2)} &amp; \cdots &amp; x_n^{(2)} \\\vdots &amp; \vdots &amp; \cdots &amp; \vdots \\x_1^{(m)} &amp; x_2^{(m)} &amp; \cdots &amp; x_n^{(m)}\end{matrix}\right]\tag{5}\end{equation} 假设有$k$个类，分别为：$C_1, C_2, \cdots, C_k$。k-means算法目标是使得每一个样本$X^{(i)}$被划分到最相似的类别中，利用每个类别中的样本重新计算聚类中心$C_k$： \begin{equation}C_k^\prime = \frac{ \sum_{X^{(i)} \in C_k} X^{(i)}} {\#(X^{(i)} \in C_k)}\tag{6}\end{equation} 其中$\sum_{X^{(i)} \in C_k} X^{(i)}$表示的是$C_k$类中的所有样本的特征向量的和，$\#(X^{(i)} \in C_k)$表示的是类别$C_k$中的样本个数。 k-means算法停止条件是最终的聚类中心不再改变。此时，所有样本被划分到了最近的聚类中心所属的类别中。\begin{equation}min \sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij} \left|\left| X^{(i)} -C_j \right|\right|^2\tag{7}\end{equation}其中，样本$X^{(i)}$是数据集$X_{m \times n}$的第$i$行。$C_j$表示的是第$j$个类别的聚类中心。假设$M_{k \times n}$为$k$个聚类中心构成的矩阵。矩阵$Z_{n \times k}$是由$z_{ij}$构成的$0-1$矩阵，$z_{ij}$为：\begin{equation}z_{ij} = \left\{\begin{aligned}&amp;1, X^{(i)} \in C_j \\&amp;0, otherwise\end{aligned}\right.\tag{8}\end{equation} \begin{equation}\begin{aligned}\because \sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij} \left|\left| X^{(i)} -C_j \right|\right|^2&amp;= (X^{(i)} - C_j)(X^{(i)} - C_j)^T \\&amp;= \sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij} \left( (X^{(i)})(X^{(i)})^T - 2X^{(i)}C_j^T + C_jC_j^T \right) \\&amp;= \sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij} (X^{(i)})(X^{(i)})^T - 2\sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij}X^{(i)}C_j^T + \sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij}C_jC_j^T \\&amp;= \sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij} \left|\left| X^{(i)} \right|\right|^2 - 2\sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij} \sum_{t = 1}^n X_t^{(i)}C_{jt} + \sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij} \left|\left| C_j \right|\right|^2\end{aligned}\tag{9}\end{equation}又：\begin{equation}\because \sum_j z_{ij} = 1\tag{10}\end{equation}即每一个样本$X^{(i)}$只属于一个类别。则：\begin{equation}\begin{aligned}\sum_{i = 1}^{m} \sum_{j = 1}^{k} z_{ij} \left|\left| X^{(i)} -C_j \right|\right|^2 &amp;=\sum_{i = 1}^{m} \left|\left| X^{(i)} \right|\right|^2 - 2\sum_{i = 1}^{m} \sum_{t = 1}^n X_t^{(i)} \sum_{j = 1}^n z_{ij} C_{jt} + \sum_{j = 1}^{k} z_{ij} \left|\left| C_j \right|\right|^2 m_j \\&amp;= tr(XX^T) - 2 \sum_{i}\sum_{t}X_{it}(ZM)_{it} + \sum\left|\left| C_j \right|\right|^2 \\&amp;= tr(XX^T) - 2\sum_i \left( X \cdot(ZM)^T \right)_{ii} + \sum\left|\left| C_j \right|\right|^2 \\&amp;= tr(XX^T) -2tr(X\cdot (ZM)^T) + \sum\left|\left| C_j \right|\right|^2\end{aligned}\tag{11}\end{equation}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典排序算法]]></title>
    <url>%2F2019%2F05%2F17%2F%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[十大排序算法的时间复杂度（最好情况，最坏情况，平均情况），空间复杂度，排序方式以及稳定性。 排序算法 平均时间复杂度 最好情况 最坏情况 空间复杂度 排序方式 稳定性 冒泡排序 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ In-place 稳定 选择排序 $O(n^2)$ $O(n^2)$ $O(n^2)$ $O(1)$ In-place 不稳定 插入排序 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ In-Place 稳定 希尔排序 $O(n\log n)$ $O(n \log^2 n$ $O(n \log^2 n)$ $O(1)$ In-place 不稳定 归并排序 $O(n\log n)$ $O(n\log n)$ $O(n\log n)$ $O(n)$ Out-place 稳定 快速排序 $O(n\log n$) $O(n\log n)$ $O(n^2)$ $O(\log n)$ In-place 不稳定 堆排序 $O(n\log n$) $O(n\log n)$ $O(n\log n)$ $O(1)$ In-place 不稳定 计数排序 $O(n+k)$ $O(n+k)$ $O(n + k)$ $O(k)$ Out-place 稳定 桶排序 $O(n + k)$ $O(n + k)$ $O(n^2)$ $O(n+k)$ Out-place 稳定 基数排序 $O(n\times k)$ $O(n\times k)$ $O(n\times k)$ $O(n+k)$ Out-place 稳定 注意：上表中的$k$表示桶的数量，$n$表示数据规模。In-place表示占用常数内存，不占用额外内存。Out-place表示占用额外内存。 冒泡排序算法原理BubbleSort重复地走访要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。 算法动图图1 BubbleSort C++源码1234567891011121314151617181920void BubbleSort(vector&lt;int&gt;&amp; arr)&#123; if (arr.empty()) &#123; return; &#125; for (int i = 0; i &lt; arr.size(); i++) &#123; for (int j = 0; j &lt; arr.size() - 1 - i; j++) &#123; if (arr[j + 1] &lt; arr[j]) &#123; int tmp = arr[j + 1]; arr[j + 1] = arr[j]; arr[j] = tmp; &#125; &#125; &#125;&#125; 选择排序算法原理SelectionSort首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 算法动图图2 SelectionSort C++源码123456789101112131415161718192021void SelectionSort(vector&lt;int&gt;&amp; arr)&#123; if (arr.empty()) &#123; return; &#125; for (int i = 0; i &lt; arr.size(); i++) &#123; int minIndex = i; for (int j = i; j &lt; arr.size(); j++) &#123; if (arr[j] &lt; arr[minIndex]) &#123; minIndex = j; &#125; &#125; int tmp = arr[minIndex]; arr[minIndex] = arr[i]; arr[i] = tmp; &#125;&#125; 插入排序算法原理InsertionSort的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 算法动图图3 InsertionSort C++源码12345678910111213141516171819void InsertionSort(vector&lt;int&gt;&amp; arr)&#123; if (arr.empty()) &#123; return; &#125; int cur&#123;&#125;; for (int i = 0; i &lt; arr.size() - 1; i++) &#123; cur = arr[i + 1]; int preIndex = i; while (preIndex &gt;= 0 &amp;&amp; cur &lt; arr[preIndex]) &#123; arr[preIndex + 1] = arr[preIndex]; preIndex--; &#125; arr[preIndex + 1] = cur; &#125;&#125; 希尔排序算法原理ShellSort也是一种插入排序，它把记录按下表的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。 算法图示图4 ShellSort C++源码12345678910111213141516171819202122void ShellSort(vector&lt;int&gt;&amp; arr)&#123; int len = arr.size(); int tmp, gap = len / 2; while (gap &gt; 0) &#123; for (int i = gap; i &lt; len; i++) &#123; tmp = arr[i]; int preIndex = i - gap; while (preIndex &gt;= 0 &amp;&amp; arr[preIndex] &gt; tmp) &#123; arr[preIndex + gap] = arr[preIndex]; preIndex -= gap; &#125; arr[preIndex + gap] = tmp; &#125; gap /= 2; &#125;&#125; 归并算法算法原理算法采用分治法（Divide and Conquer）。它首先把长度为$n$的输入序列分成两个长度为$\frac{n}{2}$的子序列。然后将已有序的子序列(单个元素自然有序)合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 算法动图图5 MergeSort C++源码1234567891011121314151617181920212223242526272829303132333435363738vector&lt;int&gt; MergeSort(vector&lt;int&gt;&amp; arr)&#123; if (arr.size() &lt; 2) &#123; return arr; &#125; int mid = arr.size() / 2; vector&lt;int&gt; left(arr.begin(), arr.begin() + mid); vector&lt;int&gt; right(arr.begin() + mid, arr.end()); vector&lt;int&gt; t1 = MergeSort(left); vector&lt;int&gt; t2 = MergeSort(right); return merge(t1, t2);&#125;vector&lt;int&gt; merge(vector&lt;int&gt;&amp; left, vector&lt;int&gt;&amp; right)&#123; vector&lt;int&gt; result(left.size() + right.size()); for (int index = 0, i = 0, j = 0; index &lt; result.size(); index++) &#123; if (i &gt;= left.size()) &#123; result[index] = right[j++]; &#125; else if (j &gt;= right.size()) &#123; result[index] = left[i++]; &#125; else if (left[i] &gt; right[j]) &#123; result[index] = right[j++]; &#125; else &#123; result[index] = left[i++]; &#125; &#125; return result;&#125; 快速排序算法原理通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 算法动图图5 QSort C++源码]]></content>
      <categories>
        <category>数据结构与算法分析</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow API]]></title>
    <url>%2F2019%2F05%2F16%2Ftensorflow-API%2F</url>
    <content type="text"><![CDATA[1 tf.sequence_mask(lengths, maxlen=None, dtype=tf.bool, name=None)： 返回表示每个单元的前$N$个位置的mask张量。参数： lengths： 整型张量，张量的值$&lt;=maxlen$ maxlen：数值型整型张量，是返回张量最后一维的大小。默认的是lengths中的最大值 dtype：结果张量的类型 name：op的名字 例1：指定maxlen123456&gt;&gt;&gt; ins1 = tf.sequence_mask([1, 3, 2], 5)&gt;&gt;&gt; sess.run(ins1)array([[ True, False, False, False, False], [ True, True, True, False, False], [ True, True, False, False, False]])&gt;&gt;&gt; 例2：使用默认maxlen12345678&gt;&gt;&gt; ins2 = tf.sequence_mask([[1, 3], [2, 0]])&gt;&gt;&gt; sess.run(ins2)array([[[ True, False, False], [ True, True, True]], [[ True, True, False], [False, False, False]]])&gt;&gt;&gt; 例3：改变结果张量默认的dtype=tf.bool为dtype=tf.float32123456&gt;&gt;&gt; ins3 = tf.sequence_mask([1, 3, 2], 5, tf.float32)&gt;&gt;&gt; sess.run(ins3)array([[1., 0., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 0., 0., 0.]], dtype=float32)&gt;&gt;&gt; 2 tf.nn.embedding_lookup(params, ids, partition_strategy=’mod’, name=None, validate_indices=True, max_norm=None)：用于在params中对张量列表进行并行查找。该API是tf.gather()的泛化，其中的params解释为大型嵌入张量的一个划分。params可以是一个PartitionedVariable，通过使用带有分区器的tf.get_variable()返回。如果len(params) &gt; 1，则根据partition_strategy在params的元素之间划分ids的每个元素id。在所有策略中，如果id空间不均匀地划分分区数，则将为第一个(max_id + 1)%len(params)分区中的每个分配一个id。 如果partition_strategy是“mod”，我们将每个id分配给分区p = id % len(params)。例如，13个id被划分到5个分区中，如下所示:[[0,5,10]，[1,6,11]，[2,7,12]，[3,8]，[4,9]]。 如果partition_strategy是“div”，则以连续的方式将id分配给分区。在本例中，13个id被分割到5个分区，分别为:[[0,1,2]，[3,4,5]，[6,7,8]，[9,10]，[11,12]]。 查找的结果被连接成一个稠密张量。返回的张量具有shape shape(ids) + shape(params)[1:]。 参数： params:一个表示完整嵌入张量的张量，或者一个除第一维外形状相同的P张量列表，表示分片嵌入张量。或者，一个PartitionedVariable，由沿着维度0分区创建。对于给定的partition_strategy，每个元素的大小必须适当。 ids:类型为int32或int64的张量，包含要在params中查找的id。 partition_strategy:指定分区策略的字符串，与len(params) &gt; 1相关。目前支持“div”和“mod”。默认设置是“mod”。 name:操作的名称(可选)。 validate_indices:弃用。如果将此操作分配给CPU，则始终验证索引中的值是否在范围内。如果分配给GPU，超界索引会导致安全但未指定的行为，这可能包括引发错误。 max_norm: 如果非None，如果l2范数大于这个值，则每个嵌入都将被剪切 3 (类)tf.contrib.rnn.LSTMCell(或tf.nn.rnn_cell.LSTMCell)。注意：该LSTMCell没有针对性能进行优化。要使其在GPU上有更更好的性能，可以使用tf.contrib.cudnn_rnn.CudnnLSTM；要使其在CPU上获得更好性能，可以使用tf.contrib.rnn.LSTMBlockCell和tf.contrib.rnn.LSTMBlockFusedCell。 （1）init(num_units, use_peepholes=False, cell_clip=None, initializer=None, num_proj=None, proj_clip=None, num_unit_shards=None, num_proj_shards=None, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None, name=None, dtype=None, kwargs)**参数： num_units：int，LSTM Cell的单元数。 use_peepholes：bool,设置为True以启用对角/窥视孔连接。 cell_clip：（可选的），float值，如果提供了该值，Cell的状态会先于Cell的输出激活被裁剪。 initializer：（可选的），权重与投影矩阵的初始化器。 num_proj：（可选的），int，投影矩阵的输出维度。如果是None，不进行投影。 proj_clip：（可选的），float值，如果$num_proj&gt;0$并且提供了$proj_clip$，那么，投影值会被逐元素裁剪到$[-proj\_clip, proj\_clip]$范围内。 num_unit_shards：弃用 num_proj_shards：弃用 forget_bias：为了减少训练开始时的遗忘程度，遗忘门的偏置被初始化为1。从CudnnLSTM训练过的检查点恢复时必须手动将其设置为0.0。 state_is_tuple：如果为True，接受状态和返回状态是c_state和m_state的2元组。如果为False，则沿着列轴连接它们。后一种行为将很快被摒弃。 activation：内部状态的激活函数。默认值:$\tanh$。它也可以是Keras激活函数名称中的字符串。 reuse：(可选)描述是否在现有范围内重用变量的Python布尔值。如果不为真，并且现有范围已经具有给定的变量，则会引发错误。 name：String，层的名称。具有相同名称的层将共享权重，但是为了避免错误，在这种情况下需要reuse=True。 dtype：层的默认dtype(默认为None意味着使用第一个输入的类型)。在调用之前call build时必需。]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>神经网络框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACL2019笔记——A Unified Linear-Time Framework for Sentence-Level Discourse Parsing]]></title>
    <url>%2F2019%2F05%2F15%2FACL2019%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94A-Unified-Linear-Time-Framework-for-Sentence-Level-Discourse-Parsing%2F</url>
    <content type="text"><![CDATA[注：该笔记不是对论文原文的翻译，而是对论文理解的一种表述。 摘要本文提出了一种根据修辞结构理论（Rhetorical Structure Theory, RST）的有效的神经框架，用于句子级的。该框架由篇章分割器和篇章解析器组成。篇章分割器用于识别基本的篇章单元（Elementary Discourse Unit, EDU）;篇章解析器以一种自顶向下的方式构建篇章树（Discourse Tree, DT）。篇章分割器和篇章解析器都是基于Pointer Network的，并以线性时间运作。篇章分割器获得了$95.4\%$的$F1$得分，而篇章解析器获得了$81.7\%$的$F1$得分（人类的得分分别是$98.3\%$和$83.0\%$）。]]></content>
      <categories>
        <category>自然语言理解</category>
      </categories>
      <tags>
        <tag>篇章解析</tag>
        <tag>文本一致性分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共指消解——mention-ranking模型]]></title>
    <url>%2F2019%2F05%2F15%2F%E5%85%B1%E6%8C%87%E6%B6%88%E8%A7%A3%E2%80%94%E2%80%94mention-ranking%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[共指消解是要找到一段文本中指向相同实体的所有表述（mention，提及）。 共指消解的基本概念 照应语（anaphor）： 是一个指向真实实体的词； 先行语（antecedent）：真正的实体； 回指（anaphora）:照应语放在先行语后边； 预指：照应语在先行语前边。 共指消解的应用领域信息抽取（Information Extraction，IE）：共指消解可以帮助IE系统对文本中出现的提及（mention）进行归类，避免提取冗余信息；文本摘要（Text Summarization）: 共指消解可以向文本摘要系统提供文档中所有的共指关系，将所有指向同一个实体的提及根据它们在文中出现的先后顺序构成一条共指链，这条共指链有助于摘要系统提取关键信息；问答（Question Answer，QA）：共指消解能够在问题和答案上优化QA系统。首先，它能够通过分析问题中的共指关系，找到问题的核心实体；其次，共指消解能够帮助判断候选答案与问题中核心实体的相关性，从而辅助对候选答案的排序；机器翻译（Machine Translation）：共指消解可以识别文本中的代词，然后把它们归类到相应的实体中。机器翻译系统翻译文本时，就可以结合代词和实体名称来进行合理的翻译。 基于mention-ranking模型的指代消解（1）找出所有的mention如图1： 图1 找出所有的mention（2）对于某个词（这里以“my”示例），让其与该词前面的mention构成一个pair，并为每一个pair打分。如图2图2 创建pair这里的NEW代表该词没有指代。（3）选取拥有最高得分的pair作为的最终的共指关系。如图3图3 选取拥有最高得分的pair作为的最终的共指关系]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>共指消解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[循环神经网络]]></title>
    <url>%2F2019%2F05%2F08%2F%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[RNN的结构RNN的折叠结构与展开结构如图1所示。 图1 RNN的折叠结构与展开结构RNN的Cell如图2所示。图2 RNN单元 序列经过RNN通过式(1)处理：\begin{equation}\begin{split}h_t = \tanh (W \cdot [h_{t-1}, x_t] + b_0)\end{split}\tag{1}\end{equation}RNN是在随时间传递的神经网络，其深度即为时间的长度。因此，随着时间变长，神经网络逐渐加深，于是当将BPTT应用于较深的RNN时，会产生“梯度消失”问题。 梯度消失:在训练神经网络过程中，神经网络权重的更新值与误差函数梯度成正比。然而在某些情况下（如RNN等这类超深的神经网络中），梯度值会几乎消失，使得权重无法更新，甚至导致神经网络完全无法继续训练。 这种现象称为梯度消失。为解决“梯度消失”问题，研究者改进了RNN，从而产生了LSTM。 LSTM的结构LSTM的结构如图3所示： 图3 LSTM单元 LSTM的表达式：\begin{equation}\begin{split}f_t &amp;= \sigma(W_f \cdot [h_{t - 1}, x_t] + b_f) \\i_t &amp;= \sigma(W_i \cdot [h_{t - 1}, x_t] + b_i) \\\overset{\sim}{C}_t &amp;= \tanh(W_C \cdot [h_{t - 1}, x_t] + b_C) \\C_t &amp;= f_t \times C_{t-1} + i_t \times \overset{\sim}{C}_t \\o_t &amp;= \sigma(W_o \cdot [h_{t - 1}, x_t] + b_o) \\h_t &amp;= o_t \times \tanh(C_t)\end{split}\tag{2}\end{equation} LSTM有通过精心设计的称作“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。它们包含一个sigmoid神经网络层和一个pointwise乘法（点乘）操作。Sigmoid层输出0到1之间的数值，描述每个部分有多少量可以通过。 0代表“不许任何量通过”；1代表“允许任何量通过” 。LSTM 拥有三个门，来保护和控制细胞状态，分别是：遗忘门、输入门、输出门。在输出门前面会进行当前时间的细胞状态更新。每一个状态会传递一个隐藏层特征和细胞状态特征给下一个状态。 GRU的结构GRU的结构如图4所示： 图4 GRU单元 GRU与LSTM相似。与LSTM相比，GRU去除掉了细胞状态。使用隐藏状态来进行信息的传递。GRU只包含两个门：更新门和重置门。更新门的作用类似于 LSTM 中的遗忘门和输入门。它决定了要忘记哪些信息以及哪些新信息需要被添加；重置门用于决定遗忘先前信息的程度。GRU 的张量运算较少，因此它比 LSTM 的训练更快一下。GRU的表达式： \begin{equation}\begin{split}z_t &amp;= \sigma(W_z \cdot [h_{t-1}, x_t]) \\r_t &amp;= \sigma(W_r \cdot [h_{t-1}, x_t]) \\\overset{\sim}{h}_t &amp;= \tanh(W\cdot [r_t\times h_{t-1}, x_t]) \\h_t &amp;= (1 - z_t) \times h_{t-1} + z_t \times \overset{\sim}{h}_t\end{split}\tag{3}\end{equation} SRU的结构SRU的结构如图5所示： 图5 SRU单元 \begin{equation}\begin{split}\overset{\sim}{x_t} &amp;= Wx_t \\f_t &amp;= \sigma(W_f x_t + b_f) \\r_t &amp;= \sigma(W_r x_t + b_r) \\c_t &amp;= f_t \odot c_{t-1} + (1 - f_t) \odot \overset{\sim}{x}_t \\h_t &amp;= r_t \odot g(c_t) + (1 - r_t) \odot x_t\end{split}\tag{4}\end{equation}其中，f表示forget gate，r表示 reset gate，h表示output state，c表示internal state。x表示输入。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理面试常见问题]]></title>
    <url>%2F2019%2F05%2F08%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1 RNN是怎么运行的？2 什么是char-RNN，RNN结构图。3 Seq2Seq的结构。3 Attention的结构4 机器学习常用的分类算法：Logistic Regression, SVM, 决策树，随机森林等相关分类算法的原理，公式推导，模型评价，模型调参，使用场景。5 机器学习常用的聚类算法：K-Means, BDSCAN, SOM, LDA等算法的原理，模型参数的确定以及确定的方法，模型的评价，模型的使用场景（例如LDA应该确定几个主题，K-Means的k如何确定，DBSCAN密度可达与密度直达）。6 特征工程： 特征选择，特征提取，PCA降维方法中的参数主成分的确定方法，如何进行特征选择。7 Boosting和Bagging的区别。8 数据如何去除噪声，如何找到离群点，异常值，现有机器学习算法哪些可以去除噪声。9 HMM与N-gram模型之间的区别。10 梯度消失与梯度爆炸。11 奥卡姆剃须刀原理。12 TCP三次握手的原理，为什么是三次而不是其他次。13 进行数据处理时，如何过滤无用的信息，数据乱码的处理。14 交叉熵与信息熵，信息增益与信息增益率，gini系数，具体如何计算。15 BIC准则（贝叶斯信息准则）与AIC（赤池信息准则）。16 前向传播与反向传播。17 常见的损失函数。18 请列出几种文本特征提取算法。 参考答案： 文档频率、信息增益、互信息、$\chi^2$统计、TF-IDF19 简述几种自然语言处理开源工具包。 参考答案：LingPipe、FudanNLP、OpenNLP、CRF++、Standord CoreNLP、IKAnalyzer20 简述无监督和有监督算法的区别。 参考答案：（1）有监督学习：对具有概念标记（分类）的训练样本进行学习，以尽可能对训练样本集外的数据进行标记（分类）预测。这里，所有的标记（分类）是已知的。因此，训练样本的岐义性低。无监督学习：对没有概念标记（分类）的训练样本进行学习，以发现训练样本集中的结构性知识。这里，所有的标记（分类）是未知的。因此，训练样本的岐义性高。聚类就是典型的无监督学习（2）有监督学习的样本全部带标记，无监督学习的样本全部不带标记。PS:部分带标记的是半监督学习（3）训练集有输入有输出是有监督，包括所有的回归算法分类算法，比如线性回归、决策树、神经网络、KNN、SVM等；训练集只有输入没有输出是无监督，包括所有的聚类算法，比如k-means 、PCA、 GMM等21 深度学习如何提取query特征，如何利用深度学习计算语义相似度。22 算法题： 写二叉树的前序遍历，中序遍历，统计二叉树所有路径和。22 RNN为什么会梯度消失，LSTM怎么能解决梯度消失问题。23 介绍一下常见优化算法及其特点。24 Dropout的原理。25 交叉熵损失函数是什么。26 介绍一下Word2vec，CBOW和Skip-gram的区别是什么。27 GBDT和Xgboost介绍一下，并说一下区别。28 算法题：现在有词向量词典，计算句子相似度（Consine Similarity）。29 介绍一下随机森林和Xgboost，有什么区别（从bagging和boosting角度）。30 什么是SGD，什么是batch size。31 深度学习优化算法有哪些，随便介绍一个。32 现有一个神经网络和64个样本，Batch gradient descent和SGD的时间复杂度和效果比较；采用批梯度下降时，神经网络参数更新了几次。33 font color=red&gt;算法题： Two Sum问题。34 font color=red&gt;算法题：如何找到10万以内的所有质数。35 Logistic回归的损失函数怎么来的，如何进行梯度更新。36 Xgboost原理，xgboost有哪些参数，怎么调整xgboost的参数。37 现在有三枚硬币，一个是一正一反，一个是两面都是正，一个是两面都是反，现在随机抛出一枚硬币是正面，那么这枚硬币的反面也是正面的概率。 参考答案：$\frac{2}{3}$38 现在有一个比较小的数据表（包括id, score），另外有一个十分大的（上千万级别）的数据表（包括id, name），现在需要以id为索引将两张表合并，如何在O(n)时间复杂度完成。39 GRU、LSTM以及RNN的区别在什么地方。40 GBDT的损失函数是什么。41 红黑树。42 64匹马，8个跑道，选出速度最快的4匹马需要多少次。 参考答案：1143 介绍一下LSTM（介绍LSTM时候提到RNN，打断询问RNN为什么有梯度消失问题，给出具体公式）。44 LSTM用什么框架实现的，能不能介绍一下Word2ver如何使用在其中，使用Word2vec和不使用word2vec的效果如何。45 正则化方法有哪些，介绍一下（说到L1和L2时，重点问了一下为什么梯度稀疏和梯度选择，用公式推导讲了一下）。46 机器学习、数据挖掘和深度学习的区别。47 算法题：二叉搜索树的插入和搜索。48 有序循环链表中（后简化为元素从小到大有序循环链表），如何在O(1)时间内完成最大值插入。49 算法题： 写代码实现列表 [0,0,6,2,8,0,0] —-&gt; [6,2,8,0,0,0]，要求O(n)时间复杂度和O(1)空间复杂度。50 LSTM用来解决RNN的什么问题？如何解决的？既然说到forget gate，那么说一下forget gate的取值范围？（sigmoid 取值（0,1））forget gate是具体的值还是向量？（向量），如何理解这个向量？51 深度学习用的什么框架，Tensorflow？（Keras），那介绍一下深度学习中的过拟合如何解决？（从数据、单模型、模型集成三个角度回答）。52 深度学习优化算法用过哪些？讲讲Sgd和gd的区别？53 对SVM（考虑线性可分情况）、LR和DT熟悉么？ 从损失函数说一下区别，SVM的损失函数是什么？（合页损失函数，写一下讲一下）。LR呢？（利用最大似然估计得出）。又问一下SVM线性可分情况下决策边界不同位置的损失值。DT如何进行特征选择？（ID3信息增益）。介绍一下信息熵？（随机变量不确定性，度量系统稳定性） [1/3,1/3,1/3]和[1/2,1/4,1/4]哪个的信息熵大？回归任务中如何进行特征选择？（平方损失准则）。54 海量数据处理。现在有1千万行词，需要统计各个词出现的次数，目前有一台机器内存1G，磁盘100G？（海量数据处理blog的第一题，先利用Hash对原始文本进行分割（hash(word)%2000，分为2000个文件），再使用hashmap（python中的字典）在各个文件中分别统计）。55 LTR（learning to rank）。介绍一下ltr的三种方式？其中pairwise在训练时怎么做？（转化为二分类）在测试的时候怎么做？56 文本分类的项目中用到CNN没有？介绍一下CNN？那CNN在文本分类任务中卷积核和一般的图像任务中的卷积核有什么区别？57 熟悉Attention么，介绍一下。58 在你的项目中如何判断word2vec的效果好坏，如何评判对模型和结果的影响？项目中使用的xgboost是哪个版本的？谁写的？59 如何从概率角度理解AUC？二分类问题中，一个正负类比是1:1000，一个是1:100，它们的AUC和ROC有什么区别？60 了解交叉熵损失函数么？在哪个场景使用过？它和最大似然估计是什么关系？61 算法题： 覆盖字符串所有字符的最小字串。62 算法题：反转链表的前k个。63 算法题：求二叉树最大深度。64 序列模型中markov和rnn的区别。 参考答案：rnn和hmm最本质的区别在于rnn没有马尔科夫假设，因此从理论上可以考虑很久的信息；同时hmm本质上是一个概率模型，而rnn不是；此外rnn具备神经网络的拟合非线性的能力。65 算法题：字符串出现第k多的字符。66 一个过拟合模型和大量数据，如何判断这些数据有没有用？67 lr和svm的区别。 参考答案：从lr的由来讲损失函数，对于svm讲最大间隔。区别在于损失函数不同；svm只需要考虑支持向量，而lr需要考虑所有的点；svm本质上是基于距离的，因此其输出无法直接产生概率，lr输出的是其属于分类的概率；在非线性的情况下，svm使用核函数解决，而lr通常不使用核函数；svm自带正则话，因此是结构风险最小化算法。68 特征选择的方法。69 AUC是什么？就是ROC曲线下的部分，表示什么？70 算法题：快速排序。71 算法题： 一个数组中超过一半的数字。72 有一个能产生1-5的随机数的函数，怎么修改之后能够产生1-7的随机数。73 快速排序，归并排序，深度遍历和广度遍历。74 解释一下lucene原理，怎么进行中文分词，基于什么进行分词。75 算法题：a-z所有字母组合方式。76 算法题：输入一个数，输出这个整数里面最大的质数。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>面试问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主题模型]]></title>
    <url>%2F2019%2F05%2F07%2F%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[主题模型产生的背景传统的信息检索（Information Retrieve）系统中通常使用文档相似性来判别查询（Query）与海量文本的相似性来得到两者之间的关联程度。然而，这种方法只能从文字表面得出query与查询文本之间的相似度，不能够表征两者之间语义上的关联。因此，基于这种文档相似度的方法的信息检索的返回字面上高度相似但语义不相关联的结果。为了解决这一问题，能够表征语义关联的主题模型就被提出来了。在自然语言理解的任务中，可以从文本的不同粒度（字符级、词级、句子级、段级和文档级）提取语义。主题模型就是一种能够从文档级表征文本语义的模型。主题建模就是在文档集合中学习、识别和提取主题的过程。主题模型基于两个相同的假设：（1）每个文档包含多个主题；（2）每个主题含有多个单词。实际上，文档的语义由一些隐变量（或潜在变量）管理。主题模型的目标就是揭示这些潜在变量，即主题。主题塑造了文档和语料库的含义。本文介绍常用的主题模型：LSA（Latent Semantic Analysis），pLSA(probability Latent Semantic Analysis)，LDA（Latent Dirichlet Allocation）和LDA2Vec。 LSA：潜在语义分析LSA的核心思想是把我们所拥有的文档-术语矩阵分解成相互独立的文档-主题和主题-术语矩阵。对于一个给定的语料库，假设该语料库中有$m$个文档，每个文档有$n$个单词。于是我们可以由此构造规模为$m\times n$的文档-术语矩阵$A$，其中行表示一篇文档，列表示该文档存在的单词。$A$中的元素$a_{ij}$可以由两种方式来确定：（1）$a_{ij}$是$j$个词在$i$篇文档中的频度；（2）$a_{ij}$用TF-IDF（Term Frequency–Inverse Document Frequency，词频-逆文本频率指数）来表示。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。由上述方式构造出来的文档-术语矩阵是一个规模庞大但稀疏的矩阵，维度存在冗余。为了找出能够捕捉单词和文档关系的少数潜在主题，有必要降低$A$的维度。我们使用SVD（Singular Value Decomposition，奇异值分解）截断来对$A$进行降维。SVD会将$A$分解成三个独立的矩阵的乘积，即$A = U \Lambda V$。其中$Lambda$是$A$的对角阵，称为奇异值，$U$称为文档-主题矩阵，$V$称为术语-主题矩阵。截断SVD的降维方式是：选择奇异值$\Lambda$中最大的$t$个数，且只保留矩阵$U$和$V$中的前$t$列。其中$t$是一个可以根据主题数量进行选择和调整的超参数。我们以数据集20_newsgroup为例来简单体会一下LSA用来建模主题。在正式引入源码之前先介绍一下20_newsgroup数据集。20 newsgroups数据集18000篇新闻文章，一共涉及到20种话题，所以称作20 newsgroups text dataset，分为两部分：训练集和测试集，通常用来做文本分类。20种话题如下： alt.atheism comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space soc.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc LSA源码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import numpy as npimport umapimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom nltk.corpus import stopwordsfrom sklearn.datasets import fetch_20newsgroupsfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.decomposition import TruncatedSVDpd.set_option("display.max_colwidth", 200)dataset = fetch_20newsgroups(data_home="/data/cgfth/JuNB/TopicModel", shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))documents = dataset.datanews_df = pd.DataFrame(&#123;'document':documents&#125;)news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z#]", " ")news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)&gt;3]))news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())stop_words = stopwords.words('english')tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])detokenized_doc = []for i in range(len(news_df)): t = ' '.join(tokenized_doc[i]) detokenized_doc.append(t)news_df['clean_doc'] = detokenized_doc vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True)X = vectorizer.fit_transform(news_df['clean_doc'])svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)svd_model.fit(X)# 显示与话题相关的一些术语terms = vectorizer.get_feature_names()for i, comp in enumerate(svd_model.components_): terms_comp = zip(terms, comp) sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7] print("Topic "+str(i)+": ") for t in sorted_terms: print(t[0]) print(" ")# 可视化, 如图1X_topics = svd_model.fit_transform(X)embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)plt.figure(figsize=(7,5))plt.scatter(embedding[:, 0], embedding[:, 1], c = dataset.target, s = 10, # size edgecolor='none' )plt.show() 图1 LSA主题模型可视化LSA是以共现表（文档-术语矩阵）的奇异值分解形式实现的，是用于小规模数据集，可用于文档分类/聚类、同义词/多义词检索、跨语言检索。SVD的计算是一个十分耗时的过程，并且潜在语义数量$t$的选择对结果的影响很大。此外，LSA的原理简单但得到的不是概率模型，缺乏统计基础，矩阵种的负值难以解释，难以对应到现实中的概念。 pLSA: 概率潜在语义分析pLSA是基于派生自LCM（least common multiple，最小公倍数）的混合矩阵分解，属于概率图模型（probabilistic graphical model，概率图模型是用图来表示变量概率依赖关系的理论，结合概率论与图论的知识，利用图来表示与模型有关的变量的联合概率分布）中的生成模型。生成式模型的代表是：一元模型（Unigram Model）、混合一元模型、n元语法模型、隐马尔可夫模型、朴素贝叶斯模型等。判别式模型的代表是：最大熵模型、支持向量机、条件随机场、感知机模型等。pLSA 模型是有向图模型，将主题作为隐变量，构建了一个简单的贝叶斯网，采用EM（Expectation-Maximization）算法估计模型参数。多项式分布：若伯努利分布由单变量扩展到$d$维的向量$x_i$，$x_i={0,1}$且$\sum_{i = 1}^{d}x_i = 1$。进一步，假设$x_i$取$1$得概率为$\mu_i \in \sum_{i = 1}^{d} \mu_i = 1$。则：\begin{equation}\begin{split}P(x|\mu) &amp;= \Pi_{i = 1}^{d} \mu_i^{x_i} \\\mathbb{E}[X_i] &amp;= \mu_i \\D[X_i] &amp;= \mu_i(1-\mu_i)\end{split}\tag{1}\end{equation}将二项分布扩展到多项分布（multinomial distribution）。多项分布描述在$N$次独立实验中有$m_i$次$x_i = 1$的概率。\begin{equation}\begin{split}P(m_1,\cdots,m_d| N, \mu) = \frac{N!}{m_1!\cdots m_d!}\Pi_{i=1}^{d}\mu_i^{m_i}\end{split}\tag{2}\end{equation}一元模型假设每篇文档中的词出现的次数服从多项式分布（式2）。pLSA模型中引入了隐变量z作为潜在语义，并使用EM算法对潜在语义模型进行拟合。pLSA的概率图模型如图2所示。 图2 pLSA主题模型概率图 pLSA主题模型概率图参数说明： 外部大方框表示有$M$篇文档的文档集，内部小方框表示每篇文档集的文档长度为$N$。每篇文档记为$d_m(d_m \in \mathbb{D}, m = 1, 2, \cdots, M)$, 其中$\mathbb{D}$记为文档集。$z_k(z_k \in \mathbb{Z^+}, k = 1, 2, \cdots, K)$是一个隐变量，表示主题，其中$K$表示主题总数，$Z^+$表示整数集。词$w_n \in \mathbb{W}(n = 1, 2, \cdots, V)$, 其中$\mathbb{W}$表示词汇表, $V$表示词汇表的规模。给定一个文档集$\mathbb{D}$, 我们可以得到一个词-文档共现矩阵，该矩阵的每个元素$n(d_m, w_n)$表示词$w_n$在文档$d_m$中的词频。pLSA对主题建模的过程如下：（1）以概率$P(d_m)$选择一篇文档$d_m$;（2）以概率$P(z_k|d_m)$得到主题$z_k$;（3）以概率$P(w_n|z_k)$生成一个词$w_n$。pLSA的参数$\theta$是：$K \times M$个$P(z_k|d_m)$和$V \times K$个$P(w_n|z_k)$。具体地：$P(z_k|d_m)$表征的是给定文档在各个主题下的分布情况下，文档在全部主题上服从多项式分布，共$M$个，$P(w_n|z_k)$表征的是给定主题词语分布情况下，主题在全部词语上服从多项式分布， 共$K$个。根据pLSA建模的过程，我们可以得到贝叶斯网表达的联合分布为：\begin{equation}P(d_m,z_k,w_n)=P(d_m)P(z_k|d_m)P(w_n|z_k)\end{equation} 该联合分布表示的含义就是pLSA建模的过程。于是，一个词$w_n$出现在文档$d_m$的联合分布为：\begin{equation}P(d_m,w_n)=P(d_m)P(w_n|d_m)\end{equation}进一步，我们根据给定文档集$D$，生成一篇文档$\overset{\rightarrow}{w}=(w_1, w_2, \cdots, w_N)$的概率为：\begin{equation}P(\vec{w}|d_m)=\prod_{n=1}^N P(w_n|d_m)\end{equation} 条件独立性： 如果$P(X, Y|Z) = P(X|Z)P(Y|Z)$（等价地$P(X|Y,Z) = P(X|Z)$），则称事件$X,Y$对于给定事件$Z$是条件独立的。即： 当$Z$发生时，$X$发生与否与$Y$发生与否是无关的。全概率公式： 设$B_1, B_2, \cdots$是样本空间$\Omega$的一个划分, $A$为任一事件，则：\begin{equation}P(A) = \sum_{i = 1}^{\infty}P(B_i)P(A|B_i)\end{equation}因此，我们可以引入一个隐变量$z_k$(实际上，$z_k$就是主题)，在不考虑随机变量之间的随机独立性，则可以得到用$z_k$表示的$P(\vec{w}|d_m)$。\begin{equation}P(w_n|d_m)=\sum_k P(z_k|d_m)P(w_n|z_k,d_m)\end{equation}上式表明，一篇文档是根据特定主题把词聚集在一起的。pLSA的概率图模型是典型的head-to-tail情况。故当$z$已知时，$d$和$w$条件独立，即：\begin{equation}P(w_n|z_k,d_m)=P(w_n|z_k)\end{equation}进一步：\begin{equation}P(w_n|d_m)=\sum_k P(z_k|d_m)P(w_n|z_k)\end{equation}所以：\begin{equation}P(d_m,w_n)=P(d_m)\sum_k P(z_k|d_m)P(w_n|z_k)\end{equation}此时，我们就可以pLSA建模转化为从文档集$\mathbb{D}$中估计上式的参数：$P(z_k|d_m)$和$P(w_n|z_k)$。考虑最大似然函数法用来估计参数：\begin{equation}\begin{aligned}L(\theta)&amp;=\ln \prod_{m=1}^M\prod_{n=1}^N P(d_m,w_n)^{n(d_m,w_n)}\\&amp;=\sum_m\sum_n n(d_m,w_n)\ln P(d_m,w_n)\\&amp;=\sum_m\sum_n n(d_m,w_n)\ln \left[P(d_m)P(w_n|d_m)\right] \\&amp;=\sum_m\sum_n n(d_m,w_n)(\ln P(d_m)+\ln P(w_n|d_m))\\&amp;=\sum_m\sum_n n(d_m,w_n)\ln P(w_n|d_m)+\sum_m\sum_n n(d_m,w_n)\ln P(d_m)\end{aligned}\end{equation}我们进一步简化上式：对于上式的第二项$\sum_m\sum_n n(d_m,w_n)\ln P(d_m)$:因为：\begin{equation}\sum_m\sum_n n(d_m,w_n)\ln P(d_m) = \sum_{m}P(d_m)\sum_{n}n(d_m, w_n)\end{equation}显然地，对于给定文档集$\mathbb{D}$, $\sum_{n}n(d_m, w_n)$是一个常数，记为$C$。于是：\begin{equation}\begin{split}\sum_m\sum_n n(d_m,w_n)\ln P(d_m) &amp;= \sum_{m}P(d_m)\sum_{n}n(d_m, w_n) \\&amp;= C\sum_{m}P(d_m)\end{split}\end{equation}又因为$P(d_m)$为在文档集$D$中选择某个文档的概率，故$\sum_{m}P(d_m)=1$。所以：\begin{equation}\begin{split}\sum_m\sum_n n(d_m,w_n)\ln P(d_m) &amp;= \sum_{m} P(d_m) \sum_{n} n(d_m, w_n) \\&amp;= C \sum_{m} P(d_m) \\&amp;= C\end{split}\end{equation}如此一来，$L(\theta)$即为：\begin{equation}\begin{aligned}L(\theta)&amp;=\ln \prod_{m=1}^M\prod_{n=1}^N P(d_m,w_n)^{n(d_m,w_n)}\\&amp;=\sum_m\sum_n n(d_m,w_n)\ln P(d_m,w_n)\\&amp;=\sum_m\sum_n n(d_m,w_n)\ln \left[P(d_m)P(w_n|d_m)\right] \\&amp;=\sum_m\sum_n n(d_m,w_n)(\ln P(d_m)+\ln P(w_n|d_m))\\&amp;=\sum_m\sum_n n(d_m,w_n)\ln P(w_n|d_m)+\sum_m\sum_n n(d_m,w_n)\ln P(d_m) \\&amp;=\sum_m\sum_n n(d_m,w_n)\ln P(w_n|d_m) + C\end{aligned}\end{equation}要求$L(\theta)$的最大值，可转化为求$\sum_m\sum_n n(d_m,w_n)\ln P(w_n|d_m)$的最大值。我们不妨新令$L(\theta)=\sum_m\sum_n n(d_m,w_n)\ln P(w_n|d_m)$。\begin{equation}\begin{aligned}L(\theta)&amp;=\sum_m\sum_n n(d_m,w_n)\ln P(w_n|d_m)\\&amp;=\sum_m\sum_n n(d_m,w_n)\ln \bigl[\sum_k P(z_k|d_m)P(w_n|z_k)\bigr]\end{aligned}\end{equation}直接对$L(\theta)$求偏导以获取最值点的方法难以在该问题中实现。于是我们引入EM法来求解$L(\theta)$的参数。 LDA： 潜在狄利克雷分布LDA2Vec]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>主题建模</tag>
        <tag>自然语言理解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OOV问题和BPE算法]]></title>
    <url>%2F2019%2F05%2F06%2FOOV%E9%97%AE%E9%A2%98%E5%92%8CBPE%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[OOV与BPE简述自然语言处理（NLP）的许多相关任务如实体关系抽取、问答，机器翻译、阅读理解、文本摘要、实体链接等都需要对语言建模。近几年常用的语言模型有词向量，如Word2vec，Glove等。面向不同的领域和不同的任务，研究人员根据实际需要训练出来了各种各样的语言模型，如对实体链接训练了实体嵌入，对知识图谱的表示训练了知识图谱嵌入（实际上是一个三元组的向量化表示）。这些语言建模促使不同的NLP任务取得不同程度的进步。然而，词向量是一个词的有限集合的向量化表示，这些词都是常见词。故而由此而建成的语言模型（Language Mode，LM）总会存在不能满足实际应用的需求，即：目标任务中可能出现一些罕见词或是派生词，词的复数或者其他的一些组合词的规则而产生的词无法用现有词向量模型表示。所以这个问题就称之为OOV(Out-Of-Vocabulary)问题。为了解决这个问题，Rico Sennrich等人提出了BPE（Byte Pair Encoder）算法， 也叫做digram coding双字母组合编码，主要目的是为了数据压缩。算法描述为字符串里频率最常见的一对字符被一个没有在这个字符中出现的字符代替的层层迭代过程。利用BPE算法旨在发现各种介于word和character之间的词根，从而尽可能的覆盖各种各样的OOV。 BPE算法示例12345678910111213141516比如我们想编码：aaabdaaabac我们会发现这里的aa出现的词数最高（我们这里只看两个字符的频率），那么用这里没有的字符Z来替代aa：ZabdZabacZ=aa此时，又发现ab出现的频率最高，那么同样的，Y来代替ab：ZYdZYacY=abZ=aa同样的，ZY出现的频率大，我们用X来替代ZY：XdXacX=ZYY=abZ=aa最后，连续两个字符的频率都为1了，也就结束了。解码的时候，就按照相反的顺序更新替换即可。 BPE算法代码1234567891011121314151617181920212223242526import re, collectionsdef get_stats(vocab): pairs = collections.defaultdict(int) for word, freq in vocab.items(): symbols = word.split() for i in range(len(symbols)-1): pairs[symbols[i], symbols[i+1]] += freq return pairsdef merge_vocab(pair, v_in): v_out = &#123;&#125; bigram = re.escape(' '.join(pair)) p = re.compile(r'(?&lt;!\S)' + bigram + r'(?!\S)') for word in v_in: w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] return v_outvocab = &#123;'l o w &lt;/w&gt;' : 5, 'l o w e r &lt;/w&gt;' : 2, 'n e w e s t &lt;/w&gt;':6, 'w i d e s t &lt;/w&gt;':3&#125;num_merges = 10for i in range(num_merges): pairs = get_stats(vocab) best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) print(best) 输出为：12345678910('e', 's')('es', 't')('est', '&lt;/w&gt;')('l', 'o')('lo', 'w')('n', 'e')('ne', 'w')('new', 'est&lt;/w&gt;')('low', '&lt;/w&gt;')('w', 'i')]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机]]></title>
    <url>%2F2019%2F05%2F05%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分治算法]]></title>
    <url>%2F2019%2F04%2F30%2F%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>数据结构与算法分析</category>
      </categories>
      <tags>
        <tag>分而治之</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BP神经网络]]></title>
    <url>%2F2019%2F04%2F29%2FBP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[初识BPBP（Back Propagation）神经网络是整个深度学习的基础。当前，大多数深度神经网络仍然采用BP作为误差反向传播的技术。通常，BP是指具有三层网络结构的浅层神经网络，如图1。实际上，BP除了输入层和输出层外，只有1层隐藏层，所以它是浅层神经网络。 图1 BP神经网络在正式推导BP的数学表示之前，我们需要先明确一下相关数学符号的意义。如图1，我们记BP的网络层数为$n_l$，则$n_l=3$。记BP网络的第$l(l=1,2,3)$层为$L_l$。故图1中的输入层、隐层和输出层分别记为$L_1$、$L_2$和$L_3$。 记$L_l$的第$i$个神经元与$L_{l+1}$的第$j$个神经元连接的权重为$W_{ij}^{(l)}$。图1中的$1$是一个偏置项，记$L_l$的偏置项为$b^{(l)}$。 BP前向传播设$L_l$的第$i$个神经元的输入为$z_i^{(l)}$，$L_l$的第$i$神经元的输出为$a_i^{(l)}$。显然，当$l=1$时，$a_i^{l}=x_i$。假设所使用的激活函数记为$f(x)$。于是，BP的前向传播可如下式计算：隐层：\begin{equation}\begin{split}z_1^{(2)} &amp;= W_{11}^{(1)}x_1 + W_{21}^{(1)}x_2 + \cdots + W_{n1}^{(1)}x_n + b^{(1)} \\a_{1}^{(2)} &amp;= f(z_1^{(2)}) = f(W_{11}^{(1)}x_1 + W_{21}^{(1)}x_2 + \cdots + W_{n1}^{(1)}x_n + b^{(1)}) \\z_2^{(2)} &amp;= W_{12}^{(1)}x_1 + W_{22}^{(1)}x_2 + \cdots + W_{n2}^{(1)}x_n + b^{(1)} \\a_{2}^{(2)} &amp; = f(z_2^{(2)}) = f(W_{12}^{(1)}x_1 + W_{22}^{(1)}x_2 + \cdots + W_{n2}^{(1)}x_n + b^{(1)}) \\\vdots \\z_m^{(2)} &amp;= W_{1m}^{(1)}x_1 + W_{2m}^{(1)}x_2 + \cdots + W_{nm}^{(1)}x_n + b^{(1)} \\a_m^{(2)} &amp;= f(z_m^{(2)}) = f(W_{1m}^{(1)}x_1 + W_{2m}^{(1)}x_2 + \cdots + W_{nm}^{(1)}x_n + b^{(1)})\end{split}\tag{1}\end{equation}输出层：\begin{equation}\begin{split}z_1^{(3)} &amp;= W_{11}^{2}a_{1}^{(2)} + W_{21}^{(2)}a_{2}^{(2)} + \cdots + W_{m1}^{(2)}a_{m}^{(2)} + b^{(2)} \\a_{1}^{(3)} &amp;= f(z_1^{(3)}) = f(W_{11}^{2}a_{1}^{(2)} + W_{21}^{(2)}a_{2}^{(2)} + \cdots + W_{m1}^{(2)}a_{m}^{(2)} + b^{(2)}) \\z_2^{(3)} &amp;= W_{12}^{2}a_{1}^{(2)} + W_{22}^{(2)}a_{2}^{(2)} + \cdots + W_{m2}^{(2)}a_{m}^{(2)} + b^{(2)} \\a_{2}^{(3)} &amp;= f(z_2^{(3)}) = f(W_{12}^{2}a_{1}^{(2)} + W_{22}^{(2)}a_{2}^{(2)} + \cdots + W_{m2}^{(2)}a_{m}^{(2)} + b^{(2)}) \\\vdots \\z_p^{(3)} &amp;= W_{1p}^{2}a_{1}^{(2)} + W_{2p}^{(2)}a_{2}^{(2)} + \cdots + W_{mp}^{(2)}a_{m}^{(2)} + b^{(2)} \\a_p^{(3)} &amp;= f(z_p^{(3)}) = f(W_{1p}^{2}a_{1}^{(2)} + W_{2p}^{(2)}a_{2}^{(2)} + \cdots + W_{mp}^{(2)}a_{m}^{(2)} + b^{(2)})\end{split}\tag{2}\end{equation}我们将上面的输出层的式子用统一的形式表述之。即\begin{equation}\hat{y} = f(\sum_{i = 1}^{m}w_{ij}^{(2)}a_i^{(2)} + b^{(2)}), j = 1, 2, \cdots, p\tag{3}\end{equation} 反向传播设有$s$个训练样本${(X^{(1)}, y^{(1)}), (X^{(2)}, y^{(2)}), \cdots, (X^{(s)}, y^{(s)})}$。对于训练样本$(X, y)$，其损失函数为：\begin{equation}L(W, b) = \frac{1}{2} \left|| \hat{y}- y \right||^2\tag{4}\end{equation}通常，为了防止模型过拟合，损失函数会加上正则项$R$。即\begin{equation}L(W, b) = L(W, b) + R\tag{5}\end{equation}其中$R$为：\begin{equation}R = \frac{\lambda}{2}\sum_{l=1}^{n_l - 1}\sum_{i = 1}^{q_l}\sum_{j = 1}^{q_{l + 1}}(W_{ij}^{(l)})\tag{6}\end{equation}这里的$q_l$表示第$L_l$上的神经元数量。所以最终的损失函数$L$可形式化为：\begin{equation}L(w, b) = \frac{1}{s}\sum_{i = 1}^{s}L(W, b) + \frac{\lambda}{2}\sum_{l=1}^{n_l - 1}\sum_{i = 1}^{q_l}\sum_{j = 1}^{q_{l + 1}}(W_{ij}^{(l)})\tag{7}\end{equation}其中$\lambda$为正则化率。 为使损失函数$L(w, b)$得到最小值，我们需要通过梯度下降来调整权重$W$以及偏置项$b$。其数学表述如下：\begin{equation}\begin{split}W_{ij}^{(l)} &amp;= W_{ij}^{(l)} -\alpha \frac{\partial}{\partial W_{ij}^{(l)}} L(W, b) \\b_{i}^{(l)} &amp;= b_{i}^{(l)} - \alpha \frac{\partial}{\partial b_{i}^{(l)}} L(W, b)\end{split}\tag{8}\end{equation}其中$\alpha$称为学习率。我们对$(7)$式分别对$W$和$b$求偏导：\begin{equation}\begin{split}\frac{\partial}{\partial W_{ij}^{(l)}}L(W, b) &amp;= \frac{1}{s}\sum_{i = 1}^{s}\frac{\partial}{\partial W_{ij}^{(l)}}L(W, b) + \lambda W_{ij}^{(l)} \\\frac{\partial}{\partial b_i^{(l)}}L(W, b) &amp;= \frac{1}{s}\sum_{i = 1}^{s}\frac{\partial}{\partial b_i^{(l)}}L(W, b)\end{split}\tag{9}\end{equation} BP源码BP源码]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节跳动2018校招算法方向编程题第3题]]></title>
    <url>%2F2019%2F04%2F29%2F%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A82018%E6%A0%A1%E6%8B%9B%E7%AE%97%E6%B3%95%E6%96%B9%E5%90%91%E7%BC%96%E7%A8%8B%E9%A2%98%E7%AC%AC3%E9%A2%98%2F</url>
    <content type="text"><![CDATA[题目描述产品经理(PM)有很多好的idea，而这些idea需要程序员实现。现在有$N$个PM，在某个时间会想出一个idea，每个idea有提出时间、所需时间和优先等级。对于一个PM来说，最想实现的idea首先考虑优先等级高的，相同的情况下优先考虑所需时间最小的，还相同的情况下选择最早想出的，没有 PM会在同一时刻提出两个idea。 同时有$M$个程序员，每个程序员空闲的时候就会查看每个PM尚未执行并且最想完成的一个idea,然后从中挑选出所需时间最小的一个idea独立实现，如果所需时间相同则选择PM序号最小的。直到完成了idea才会重复上述操作。如果有多个同时处于空闲状态的程序员，那么他们会依次进行查看idea的操作。 求每个idea实现的时间。 输入第一行三个数$N$、$M$、$P$，分别表示有$N$个PM，$M$个程序员，$P$个idea。随后有$P$行，每行有$4$个数字，分别是PM序号、提出时间、优先等级和所需时间。输出$P$行，分别表示每个idea实现的时间点。 输入描述输入第一行三个数$N$、$M$、$P$，分别表示有$N$个PM，$M$个程序员，$P$个idea。随后有$P$行，每行有$4$个数字，分别是PM序号、提出时间、优先等级和所需时间。全部数据范围$[1, 3000]$。 输出描述输出$P$行，分别表示每个idea实现的时间点。 输入示例2 2 51 1 1 21 2 1 11 3 2 22 1 1 22 3 5 5 输出示例34539 示例解析这是一个多执行机多任务调度问题。我们记PM序号、提出时间、优先等级和所需时间相对于程序员的优先级分别记为$P_{PMSeqNum}$、$P_{PropTime}$、$P_{Pror}$和$P_{RunTime}$。显然： $$P_{Prior} &gt; P_{RunTime} &gt; P_{PMSeqNum} &gt; P_{PropTime} $$ 注意：这里的优先等级表示数字越小，则优先级越高。我们以一张表格来表示程序员实现idea的整个过程。 01 2 34 5 67 8 9 1 (1, 1, 2, 1, 1) 2 (1, 1, 1, 1, 2) 3 (2, 2, 2, 1, 2) 4 (2, 1, 2, 2, 1) 5 (1, 5, 5, 2, 3) 说明： 这里的五元组$(ID, Prior, RunTime, PMSeqNum, PropTime)$分别表示程序员的ID，idea优先等级、 运行时间、 PM序号和提出时间。绿色表示idea处于执行态，褐色表示idea处于等待状态。 整体思路重点代码讲解完整代码]]></content>
      <categories>
        <category>笔试编程</category>
      </categories>
      <tags>
        <tag>任务调度</tag>
        <tag>优先级队列</tag>
        <tag>堆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔试题]]></title>
    <url>%2F2019%2F04%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、 一个二进制$X$发出符号集为${-1, 1}$，经过离散无记忆信道传输，由于信道中噪音的存在，在接收端$Y$收到符号集为${-1, 1, 0}$。已知$P(x=-1)=\frac{1}{4}$，$P(x=1)=\frac{3}{4}$，$P(y=-1|x=1)=\frac{4}{5}$，$P(y=0|x=-1)=\frac{1}{5}$，$P(y=1|x=1)=\frac{3}{4}$，$P(y=0|x=1)=\frac{1}{4}$，求条件熵$H(Y|X)$ () A 0.2375B 0.3275C 0.5273D 0.5372 解析:\begin{split}\because\quad H(X|Y) &amp;=-\sum P(X, Y)\log P(Y|X) \\ &amp;= -\sum P(Y|X)P(X)\log P(Y|X)\end{split} \begin{split} \therefore\quad H(X|Y)&amp;=-(P(y=-1|x=-1)P(x=-1)\log P(y=-1|x=-1) \\ &amp; + P(y=0|x=-1)P(x=-1)\log P(y=0|x=-1) \\ &amp; + P(y=1|x=1)P(x=1)\log P(y=1|x=1) \\ &amp; + P(y=0|x=1)P(x=1)\log P(y=0|x=1)) \\ &amp; =-(\frac{4}{5}\times\frac{1}{4}\times\log\frac{4}{5} + \frac{1}{5}\times\frac{1}{4}\times\log\frac{1}{5} + \frac{3}{4}\times\frac{3}{4}\times\log\frac{3}{4} + \\ &amp; \frac{1}{4}\times\frac{3}{4}\times\log\frac{1}{4}) \\&amp;\approx -(-0.01938-0.03495-0.07028-0.11289)=0.2375\end{split} 特别注意： 这里的$log$是以$10$为底的对数函数。答案：A2、 下列哪个不属于CRF模型对于HMM和MEMM模型的优势A 特征灵活B 速度快C 可容纳较多上下文信息D 全局最优解析：HMM（Hidden Markov Model, 隐马尔可夫模型）是对转移概率和表现概率直接建模， 统计共现概率。MEMM（Maximum Entropy Markov Model, 最大熵马尔可夫模型）是对转移概率和发射概率建立联合概率，统计时统计的时条件概率， 但由于MEMM只在局部做归一化，故容易陷入局部最优。CRF(Conditional Random Field，条件随机场)统计了全局概率。再做归一化时考虑了数据在全局的分布，而不是仅仅只做局部归一化，这样解决了MEMM中的标记偏置（Label Bias）的问题。CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。转移概率：给定的马氏链在某时刻处于某一状态，再经若干时间达到另一状态的条件概率。表现概率：观察变量的条件概率，亦称发射概率。共现概率：答案：B3、 模式识别中，不属于马氏距离较之于欧式距离的优点是（）A 平移不变性B 尺度不变性C 考虑了模式的分布解析：马氏距离（Mahalanobis Distance）是由印度统计学家马哈拉洛比斯提出的，表示数据的协方差距离，是一种有效的计算两个未知样本集相似度的方法。与欧式距离不同的是，马氏距离考虑到各种特性之间的关系并且是尺度无关的（scale-invariant），即独立于测量尺度。对于一个均值为$\mu=(\mu_1, \mu_2, \cdots, \mu_p)^T$, 协方差矩阵为$\Sigma$的多变量向量为$x=(x_1, x_2, \cdots, x_p)^T$, 其马氏距离为：\begin{equation}D_M(x) =\sqrt{(x-\mu)^T\Sigma^{-1}(x-\mu)}\end{equation}对于两个变量$x$和$y$， 其协方差计算公式：\begin{equation}cov(x, y) = \mathbb{E}(x-\mathbb{E}(x))(y-\mathbb{E}(y))\end{equation}对于含有$n$个变量（或属性）($c_1, c_2, \cdots, c_n$)的数据集，则可得协方差矩阵：\begin{equation}\Sigma=\begin{bmatrix}cov(c_1, c_1) &amp; cov(c_1, c_2) &amp; \cdots &amp; cov(c_1, c_n) \\cov(c_2, c_1) &amp; cov(c_2, c_2) &amp; \cdots &amp; cov(c_2, c_n) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\cov(c_n, c_1) &amp; cov(c_n, c_2) &amp; \cdots &amp; cov(c_n, c_n)\end{bmatrix}\end{equation}如果协方差矩阵为单位阵，则马氏距离就是欧式距离；如果协方差矩阵为对角阵，此时称马氏距离为正规化的欧式距离。马氏距离的优点：马氏距离不受量纲影响：两点之间的马氏距离与原始数据的测量单位无关；由标准化数据和中心化数据计算的二点之间的距离相同；马氏距离可以排除变量之间的相关性的干扰。欧式距离特性有：平移不变性、旋转不变性。马式距离特性有：平移不变性、旋转不变性、尺度缩放不变性、不受量纲影响的特性、考虑了模式的分布。 答案：A 4、 以下（）不属于线性分类器最佳准则。A 感知器准则函数B 贝叶斯分类C 支持向量机D Fisher准则 解析：线性分类器有三大类：感知器准则函数、SVM、Fisher准则。贝叶斯分类器不是线性分类器。感知器准则函数：准则函数以使错分类样本到分界面距离之和最小为原则。优点是通过错分类样本提供的信息对分类器函数进行修正。支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小（使用核函数可解决非线性问题）。Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条原点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。 答案：B 5、 下面有关分类算法的准确率、召回率和$F1$值的描述，错误的是：A 准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率B 召回率是指检索出相关文档数和文档库中所有相关文档数的比率，衡量的是检索系统的查全率C 正确率、召回率和$F1$值取值都在$0$和$1$之间，数值越接近$0$，查准率和查全率越高D 为了解决准确率和召回率冲突的问题，引入了$F1$分数 解析：这里我们使用文档分类作为示例来阐述准确率、精准率、召回率、$F1$和$AUC$。假设文档库含有$n$篇文档，这些文档可分为$k$类，每类文档具有$m_k$篇文档。设有某算法可对该文档库进行分类，每类正确归档数记为$c_k(c_k &lt;= m_k)$。准确率（Accuracy）即是该算法分类正确的文档占所有文档库的比率。\begin{split}acc = \frac{\sum_{i = 1}^{k}c_k}{\sum_{i = 1}^{k}m_k}\times 100\% = \frac{\sum_{i = 1}^{k}c_k}{n} \times 100\%\end{split}其中$n=\sum_{i = 1}^{k}m_k$。在介绍其他概念之前，需要引入基于二分类的混淆矩阵。实际上，多分类问题可以划分为多个二分类问题。具体地，在多分类问题中，对某一类别$k_i(i=1,2,\cdots,k)$, 若该算法得到的类别标签属于该类别，则分类正确；若由算法得到的分类标签属于其他类别，则分类错误。由此得到了$k_i$类和其他类的二分类问题。 混淆矩阵定义如下： 预测值0 预测值1 真实值0 TN FP 真实值1 FN TP 我们将当前所关注的分类类别的数据项称之为正例，而不受当前关注的类别的数据项称之为负例。则有：TN：算法预测为负例（N），实际上也是负例（N）的个数，即算法预测对了（True）；​FP：算法预测为正例（P），实际上是负例（N）的个数，即算法预测错了（False）；​FN：算法预测为负例（N），实际上是正例（P）的个数，即算法预测错了（False）；​TP：算法预测为正例（P），实际上也是正例（P）的个数，即算法预测对了（True）。于是精准率(Precision)、召回率(Recall)和$F1$值定义如下：\begin{equation}prec = \frac{TP}{TP+FP}\end{equation} \begin{equation}recall = \frac{TP}{TP+FN}\end{equation} \begin{equation}F1 = \frac{2\times prec \times recall}{prec + recall}\end{equation} AUC被定义为ROC曲线下的面积。诸如逻辑回归这样的分类算法而言，通常预测的都是一个概率值，我们会认为设置一个阈值，超过这个阈值，就预测为其中一类，不超过这个阈值，定义为另外一类。于是，不同的阈值就对应了不同的假正率和真正率，于是通过不同的阈值就形成了假正率和真正率序列，它们就可以在直角坐标系上通过描点成为光滑曲线，这个曲线就是 ROC 曲线。横坐标：假正率（False positive rate， FPR），预测为正但实际为负的样本占所有负例样本的比例。\begin{equation}FPR=\frac{FP}{TN+FP}\end{equation}纵坐标：真正率（True positive rate， TPR），这个其实就是召回率，预测为正且实际为正的样本占所有正例样本的比例。\begin{equation}TPR=\frac{TP}{TP+FN}\end{equation} 答案：C 6、 一下几种模型方法属于判别式模型（Discriminative）的有（）（1）混合高斯模型（2）条件随机场模型（3）区分度模型（4）隐马尔可夫模型A 2,3B 3,4C 1,4D 1,2解析:常见的判别式模型有：Logistic Regression（LR，逻辑回归）Linear Discriminative Analysis(LDA，线性判别分析, Fisher准则)Support Vector Machine(SVM，支持向量机)Boosting（集成学习）Conditional Random Field（CDF, 条件随机场）Linear Regression（LR, 线性回归）Neural Networks(NN, 神经网络)常见的生成模型：Gaussian Mixture Model（GMM，高斯混合模型以及其他类型的混合模型）Hidden Markov Model（HMM，隐马尔可夫模型）Naive Bayies（朴素贝叶斯）Averaged One-Dependence Estimators （AODE，平均单依赖估计）Latent Dirichlet Allocation（LDA主题模型）Restricted Boltzmann Machine(RBM，受限玻尔兹曼机)答案：A 7、你正在使用带有L1正则化的logistic回归做二分类，其中$C$是正则化参数，$w_1$和$w_2$是$x_1$和$x_2$的系数。当你把$C$值从$0$增加到非常大的值时，下面哪个选项是正确的？A 第一个$w_2$成了$0$，接着$w_1$也成了$0$B 第一个$w_1$成了$0$，接着$w_2$也成了$0$C $w_1$和$w_2$同时成了$0$D 即使在$C$成为最大值之后，$w_1$和$w_2$都不能成为$0$解析：通过图1可以看出$w_1$和$w_2$同时成了$0$ import numpy as np import numpy as np from matplotlib import pyplot as plt from mpl_toolkits.mplot3d import Axes3D fig = plt.figure() ax1 = Axes3D(fig) L1 = 1 W1 = np.linspace(-3, 3, 60) W2 = np.linspace(-5, 5, 60) C = L1 / (abs(W1) + abs(W2)) ax1.scatter3D(W1, W2, C) ax1.plot3D(W1, W2, C, 'red') ax1.set_title("$L1=C*(|w1|+|w2|)$") ax1.set_xlabel("$w_1$") ax1.set_ylabel("$w_2$") ax1.set_zlabel("$C$") plt.show() 图1 L1正则化函数图像答案：C 8、下列哪个不属于常用的文本分类的特征选择算法？A $\chi^2$检验值B 互信息C 信息增益D 主成分分析解析：常用的特征选择方法：（1）DF（Document Frequency，文档频率）：统计特征词出现的文档数量，用来衡量特征词的重要性；（2）MI（Mutual Information，互信息法）：用于衡量特征词与文档类关系的信息量。如果某个特征词的频率很低，那么互信息得分就会很高。反之，如果该特征词的频率很高，则互信息得分会很低。因此互信息法倾向低频的特征词；（3）IG（Information Gain，信息增益法）：在某个特征词的缺失与存在的两种情况，根据语料中前后信息的变化来衡量特征词的重要性。（4）$\chi^2$检验法：利用统计学中“假设检验”的基本思想。首先假设特征词与类别不直接相关，如果利用$\chi^2$分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备择假设：特征词与类别有着很高的关联度。（5）WLLR（Weighed Log Likelihood Ration，加权对数似然比）；（6）WFO（Weighted Frequency and Odds）:加权频率和可能性。答案： D 9、下列不是SVM核函数的是（）A 多项式和函数B logistic核函数C 径向基核函数D Sigmoid核函数 解析：SVM核函数包括线性核函数、多项式核函数、径向基(RBF)核函数（Guassian核函数）、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。根据泛函有关理论，只要一种函数$\kappa(x_i, x_j)$满足Mercer条件，它就对应某一种变换空间的内积。Mercer定理：任何半正定的函数都可以作为核函数。所谓半正定的函数$f(x_i, x_j)$，是指对训练数据的集合$(x_1, x_2, \cdots, x_n)$定义一个由元素$a_{ij}=f(x_i, x_j)$组成的矩阵。显然，这个矩阵的规模是$n \times n$。如果该矩阵是半正定的，则$f(x_i, x_j)$就称为半正定函数。常见的核函数：（1）线性核函数\begin{equation}\kappa(x, z) = &lt;x, z&gt;\end{equation}（2）多项式核函数\begin{equation}\kappa(x, z) = (&lt;x, z&gt; + 1)^d, r\in Z^+\end{equation}（3）RBF核函数\begin{equation}\kappa(x, z) = e^{-\frac{\left||x-z\right||^2}{2\sigma^2}}, \sigma=R-\{0\}\end{equation}（4）幂指数核函数\begin{equation}\kappa(x, z) = e^{-\frac{\left||x - z\right||}{2\sigma^2}}\end{equation}（5）拉普拉斯核函数\begin{equation}\kappa(x,z) = e^{-\frac{\left||x - z \right||}{\sigma}}\end{equation}（6）ANOVA核函数\begin{equation}\kappa(x, z) = \sum_{k = 1}^n e\left(\sigma(x^k - z^k)^2 \right) ^ d\end{equation}（7）二次有理核函数\begin{equation}\kappa(x, z) = 1 - \frac{\left|| x - z \right||^2}{\left||x - z \right||^2 + c}\end{equation}（8）多元二次核函数\begin{equation}\kappa(x, z) = \sqrt{\left|| x - z \right||^2 + c^2}\end{equation}（9）逆多元二次核函数\begin{equation}\kappa(x, z) = \frac{1}{\sqrt{\left|| x - z \right||^2 + c^2}}\end{equation}（10）sigmoid核函数\begin{equation}\kappa(x, z) = \tanh(\alpha x^T + c)\end{equation}（11）傅立叶核函数\begin{equation}\kappa(x_i, z_j) = \frac{1}{2} + \sum_{k=1}^{N}(\sin(kx_i)\sin(kz_j) + \cos(kx_i)\cos(kz_j))\end{equation} 注：$d$表示阶数。$d$越大，映射的维度越高，计算量越大，此时容易出现“过拟合现象”。采用sigmoid作为核函数时，支持向量机实际就是一种多层感知器神经网络。此时，神经网络的隐含层节点数目、隐含层节点对输入节点的权值都是训练过程中自动确定的。支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最优值，也保证了它对于未知样本的良好泛化性能而不会出现“过拟合”现象。核函数的选择：在选择核函数解决实际问题时，通常采用的方法有：（1）利用专家的先验知识预先选定核函数；（2）采用交叉验证（Cross-validation）方法。即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数。如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多。（3）采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想。答案：B]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[math]]></title>
    <url>%2F2019%2F04%2F27%2Fmath%2F</url>
    <content type="text"><![CDATA[$\frac{x}{y}$]]></content>
  </entry>
  <entry>
    <title><![CDATA[强化学习]]></title>
    <url>%2F2019%2F04%2F27%2F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[\begin{equation}f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu^)2}{z\sigma^2}}\end{equation}]]></content>
  </entry>
  <entry>
    <title><![CDATA[NLP]]></title>
    <url>%2F2019%2F04%2F27%2FNLP%2F</url>
    <content type="text"><![CDATA[1、 一个二进制$X$发出符号集为${-1, 1}$，经过离散无记忆信道传输，由于信道中噪音的存在，在接收端$Y$收到符号集为${-1, 1, 0}$。已知$P(x=-1)=\frac{1}{4}$，$P(x=1)=\frac{3}{4}$，$P(y=-1|x=1)=\frac{4}{5}$，$P(y=0|x=-1)=\frac{1}{5}$，$P(y=1|x=1)=\frac{3}{4}$，$P(y=0|x=1)=\frac{1}{4}$，求条件熵$H(Y|X)$ () A 0.2375B 0.3275C 0.5273D 0.5372 解析:\begin{split}\because\quad H(X|Y) &amp;=-\sum P(X, Y)\log P(Y|X) \\ &amp;= -\sum P(Y|X)P(X)\log P(Y|X)\end{split} \begin{split} \therefore\quad H(X|Y)&amp;=-(P(y=-1|x=-1)P(x=-1)\log P(y=-1|x=-1) \\ &amp; + P(y=0|x=-1)P(x=-1)\log P(y=0|x=-1) \\ &amp; + P(y=1|x=1)P(x=1)\log P(y=1|x=1) \\ &amp; + P(y=0|x=1)P(x=1)\log P(y=0|x=1)) \\ &amp; =-(\frac{4}{5}\times\frac{1}{4}\times\log\frac{4}{5} + \frac{1}{5}\times\frac{1}{4}\times\log\frac{1}{5} + \frac{3}{4}\times\frac{3}{4}\times\log\frac{3}{4} + \\ &amp;\frac{1}{4}\times\frac{3}{4}\times\log\frac{1}{4}) \\&amp;\approx -(-0.01938-0.03495-0.07028-0.11289)=0.2375\end{split} 特别注意： 这里的$log$是以$10$为底的对数函数。答案：A]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识图谱]]></title>
    <url>%2F2019%2F04%2F27%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[随机森林]]></title>
    <url>%2F2019%2F04%2F27%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[随机森林（Random Forest, RF）算法是一种重要的基于Bagging的集成学习方法。随机森林可以用来解决分类、回归等问题。在正式介绍随机森林之前，需要介绍集成学习的相关概念。 集成学习集成学习的思想就是要通过训练多个分类器来共同解决一个复杂分类问题。在集成学习方法中，其泛化能力比单个学习算法的泛化能力强很多。根据多个分类器学习方式的不同，集成学习方法可以分为Bagging算法和Boosting算法。（1）Bagging（Bootstrap Aggregating）算法通过对训练样本有放回的抽取，由此产生多个训练数据子集，并在每个训练数据子集上训练一个分类器。最终的分类结果由多个分类器投票产生。（2）Boosting算法通过顺序地给训练集中的数据项重新加权创造创造不同的基础学习器。训练开始时，所有的数据项都被初始化为同一个权重。之后，每次增强的迭代都会生成一个适应加权之后的训练数据集的基础学习器。每一次迭代的错误率都会计算出来，正确划分的数据项的权重会被降低，而被错误划分的数据项权重将会增大。Boosting算法的最终模型是一系列基础学习器的线性组合，而且系数依赖于各个基础学习器的表现。 随机森林随机森林算法由一系列决策树组成。 通过Bootstrap重采样技术，从原始训练样本集中有放回地重复随机抽样$m$个样本，生成新的训练样本集合，然后根据自助样本集生成$k$个分类树组成随机森林。新数据的分类结果根据决策树投票形成的分数决定。这里我们采用CART分类树作为随机森林的决策树。CART算法是决策树（主要的决策树模型有ID3算法、C4.5算法和CART算法）的一种。于ID3和C4.5不同的是，CART既可以用来解决分类问题，也可以用来处理回归问题。在CART分类树算法中，利用Gini指数作为划分数据集属性的指标。设有一数据集，记为$D$。假设有$K$个分类，样本属于第$k$个类的概率为$p_k$，则此概率的Gini指数为：\begin{equation}Gini(p) = \sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2\tag{1}\end{equation}对于数据集$D$，其Gini指数为：\begin{equation}Gini(D) = 1 - \sum_{k=1}^{K}\left(\frac{|C_k|}{|D|}\right)^2\tag{2}\end{equation}其中$|C_k|$表示数据集$D$中，属于类别$k$的样本个数。若根据特征$A$将数据集$D$划分成独立的两个数据集$D_1$和$D_2$，则此时Gini指数为:\begin{equation}Gini\left(D, A\right) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)\tag{3}\end{equation}在划分数据属性时，需要设置划分终止条件。通常终止条件可以是：（1 节点中的样本数小于给定阈值；（2）样本集的Gini指数小于给定阈值（样本基本属于同一类）；（3）没有更多特征 CART分类树的构建流程如下：（1）遍历训练数据集的所有属性和可能的切分点，寻找最佳切分属性及其最佳切分点，使得切分之后的Gini系数最小；（2）利用找到的最佳属性极其最佳切分点将训练数据集切分成两个子集，分别对应分类树中的左子树和右子树；（3）重复以上步骤（为每一个叶子节点寻找最佳切分属性及其最佳切分点，将其划分为左右字数）知道满足终止条件；（4）生成CART树。 随机森林算法是通过训练多个决策树得到综合的分类模型，其只要两个参数：构建决策树的个数$n_{tree}$和在决策树的每个节点进行分裂时需要考虑的输入特征个数$k$。其中$k$可以取$\log_2n$。这里的$n$表示训练数据集中特征（属性）的个数。对于单棵决策树的构架，流程如下：（1）随机有放回地从训练数据集中抽取$m$个样本；（2）从$n$样本特征中随机挑选$k$个特征，然后从这个$k$个特征中选择最好一个进行分裂（决策树的生成或构建）；（3）重复步骤（2），直到该节点的所有训练样本都属于同一类。注意:在决策树分裂过程中不需要剪支。 补充材料在决策树算法中，划分数据集除了Gini指数外，还有信息增益和增益率。引入信息增益（Information Gain）和增益率（Gain Ratio）之前，需要先介绍熵（Entropy）的概念。熵是度量集合纯度最常用一种指标。信息熵较小表明数据集纯度较高。 对于包含$m$个训练样本的数据集$D:{X^{(1)}, y^{(1)}, \cdots, (X^{(m)}, y^{(m)}}$，在数据集$D$中， 第$k$类样本所占的比例为$p_k$，则数据集$D$的信息熵$En(D)$为：\begin{equation}En(D)=-\sum_{k=1}^{K}\log_2np_k\tag{4}\end{equation}其中$K$表示数据集$D$的类别个数。当把样本按照特征$A$的划分成两个独立的数据集$D_1$和$D_2$时，此时数据集$D$的熵$En(D)$为两个独立数据集$D_1$的熵$En(D_1)$和$En(D_2)$的加权和：\begin{equation}\begin{split}En(D) &amp;= \frac{|D_1|}{|D|}En(D_1) + \frac{|D_2|}{|D|}En(D_2) \\&amp;= -\left( \frac{|D_1|}{|D|}\sum_{k=1}^{K}p_k\log_2p_k + \frac{|D_2|}{|D|}\sum_{k=1}^{K}p_k\log_2p_k \right)\end{split}\tag{5}\end{equation} 信息增益（$igain$）是对划分给定数据集前后信息熵的减少量，即：\begin{equation}igain(D, A) = En(D) - \sum_{p=1}^{P}\frac{|D_p|}{|D|}En(D_p)\tag{6}\end{equation}其中，$D_p$表示属于第$p$类的样本数。在选择数据集划分标准时，通常选择能够使得信息增益最大的划分。ID3算法是利用信息增益作为划分数据集的一种方法。增益率也可以作为选择最优划分属性的方法，C4.5就是采用增益率作为划分数据集的方法。增益率可以基于下式计算:\begin{equation}gain\_ratio(D,A) = \frac{igain(D, A)}{IV(A)}\tag{7}\end{equation}其中$IV(A)$称为$A$的固有值（Intrinsic Value）。即：\begin{equation}IV(A)=\sum_{p=1}^{P}\frac{|D_p|}{|D|}\log_2\frac{|D_p|}{|D|}\tag{8}\end{equation} 源码随机森林源码]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[random-forest]]></title>
    <url>%2F2019%2F04%2F27%2Frandom-forest%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
</search>
